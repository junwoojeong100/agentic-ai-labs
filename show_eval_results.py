#!/usr/bin/env python3
"""í‰ê°€ ê²°ê³¼ ìƒì„¸ ì¶œë ¥"""
import json
from pathlib import Path

SEPARATOR = "â”€" * 100
LINE = "=" * 100

def get_score_color(score, threshold=3.0):
    if score >= 4.5:
        return "\033[92m"
    elif score >= threshold:
        return "\033[93m"
    else:
        return "\033[91m"

def reset_color():
    return "\033[0m"

def get_score_indicator(score, threshold=3.0):
    if score >= 4.5:
        return "âœ…"
    elif score >= threshold:
        return "âš ï¸"
    else:
        return "âŒ"

def extract_query_text(query_input):
    if isinstance(query_input, list):
        for item in query_input:
            if isinstance(item, dict) and item.get("role") == "user":
                content = item.get("content", [])
                if isinstance(content, list) and len(content) > 0:
                    return content[0].get("text", "")
    return ""

def extract_response_text(response):
    if isinstance(response, list):
        for item in response:
            if isinstance(item, dict) and item.get("role") == "assistant":
                content = item.get("content", [])
                if isinstance(content, list) and len(content) > 0:
                    return content[0].get("text", "")
    return ""

def main():
    eval_output_path = Path("evals/eval-output.json")
    
    print(LINE)
    print("ğŸ“Š AGENT EVALUATION RESULTS - ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸")
    print(LINE, "\n")
    
    if not eval_output_path.exists():
        print("âŒ í‰ê°€ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   íŒŒì¼ ê²½ë¡œ: {eval_output_path.absolute()}")
        print("\n   ë¨¼ì € 06_evaluate_agents.ipynbì˜ ì…€ 5ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n")
        return
    
    with open(eval_output_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    metrics = data.get("metrics", {})
    rows = data.get("rows", [])
    
    # ì„¹ì…˜ 1: ì „ì²´ í‰ê·  ì ìˆ˜
    print("â­ ì „ì²´ í‰ê·  ì„±ëŠ¥ ì ìˆ˜")
    print(LINE)
    scores_config = [
        ("Intent Resolution", "intent_resolution.intent_resolution", "ì˜ë„ íŒŒì•…", 3.0),
        ("Task Adherence", "task_adherence.task_adherence", "ì‘ì—… ì¶©ì‹¤ë„", 3.0),
    ]
    
    for name, key, desc, threshold in scores_config:
        if key in metrics:
            score = metrics[key]
            color = get_score_color(score, threshold)
            reset = reset_color()
            indicator = get_score_indicator(score, threshold)
            stars = "â˜…" * int(score) + "â˜†" * (5 - int(score))
            bar = "â–ˆ" * int(score * 4) + "â–‘" * (20 - int(score * 4))
            
            print(f"{indicator} {name:20} {color}{score:.2f}/5.0{reset}  {stars}")
            print(f"     {desc:20} [{bar}]")
            if score < threshold:
                print(f"     {color}âš ï¸ ì„ê³„ê°’ ë¯¸ë‹¬ (ê¸°ì¤€: {threshold:.1f}){reset}")
            print()
    
    # ì„¹ì…˜ 2: ìš´ì˜ ë©”íŠ¸ë¦­
    print("\nâš¡ ìš´ì˜ ë©”íŠ¸ë¦­ (í‰ê· )")
    print(LINE)
    
    operational_keys = [
        ("operational_metrics.server-run-duration-in-seconds", "ì„œë²„ ì‹¤í–‰ ì‹œê°„", "s"),
        ("operational_metrics.client-run-duration-in-seconds", "í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰ ì‹œê°„", "s"),
        ("operational_metrics.prompt-tokens", "í”„ë¡¬í”„íŠ¸ í† í°", "tokens"),
        ("operational_metrics.completion-tokens", "ì™„ì„± í† í°", "tokens"),
    ]
    
    total_tokens = 0
    for key, desc, unit in operational_keys:
        if key in metrics:
            value = metrics[key]
            if unit == "tokens":
                print(f"  {desc:30} {int(value):>10,} {unit}")
                total_tokens += value
            else:
                print(f"  {desc:30} {value:>10.2f} {unit}")
    
    if total_tokens > 0:
        print(f"  {'ì´ í† í° ì‚¬ìš©ëŸ‰':30} {int(total_tokens):>10,} tokens")
        cost = (total_tokens / 1000) * 0.0025
        print(f"  {'ì˜ˆìƒ ë¹„ìš© (GPT-4o)':30} ${cost:>9.4f}")
    print()
    
    # ì„¹ì…˜ 3: ê°œë³„ ì¿¼ë¦¬ ìƒì„¸ ê²°ê³¼
    if rows:
        print("\nğŸ“‹ ì¿¼ë¦¬ë³„ ìƒì„¸ ê²°ê³¼")
        print(LINE)
        
        for idx, row in enumerate(rows, 1):
            query = extract_query_text(row.get("inputs.query", []))
            response = extract_response_text(row.get("inputs.response", []))
            ground_truth = row.get("inputs.metrics.ground-truth", "")
            
            print(f"\n{SEPARATOR}")
            print(f"ğŸ” Query #{idx}")
            print(SEPARATOR)
            
            print("\nğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸:")
            print(f"   {query}")
            
            if ground_truth:
                print("\nğŸ“Œ ì˜ˆìƒ ë™ì‘ (Ground Truth):")
                print(f"   {ground_truth}")
            
            if response:
                print("\nğŸ¤– Agent ì‘ë‹µ (ìš”ì•½):")
                response_preview = response[:200] if len(response) > 200 else response
                lines_shown = 0
                for line in response_preview.split("\n"):
                    if line.strip() and lines_shown < 3:
                        print(f"   {line.strip()}")
                        lines_shown += 1
                if len(response) > 200:
                    print(f"   ... (ì´ {len(response):,}ì)")
            
            print("\nğŸ“Š í‰ê°€ ì ìˆ˜:")
            
            intent = row.get("outputs.intent_resolution.intent_resolution", "N/A")
            task = row.get("outputs.task_adherence.task_adherence", "N/A")
            tool = row.get("outputs.tool_call_accuracy.tool_call_accuracy", "N/A")
            
            intent_threshold = row.get("outputs.intent_resolution.intent_resolution_threshold", 3)
            task_threshold = row.get("outputs.task_adherence.task_adherence_threshold", 3)
            
            if isinstance(intent, (int, float)):
                color = get_score_color(intent, intent_threshold)
                reset = reset_color()
                indicator = get_score_indicator(intent, intent_threshold)
                print(f"   {indicator} Intent Resolution:  {color}{intent:.1f}/5.0{reset} (ì„ê³„ê°’: {intent_threshold})")
            else:
                print(f"   â€¢ Intent Resolution:  {intent}")
            
            if isinstance(task, (int, float)):
                color = get_score_color(task, task_threshold)
                reset = reset_color()
                indicator = get_score_indicator(task, task_threshold)
                print(f"   {indicator} Task Adherence:     {color}{task:.1f}/5.0{reset} (ì„ê³„ê°’: {task_threshold})")
            else:
                print(f"   â€¢ Task Adherence:     {task}")
            
            print(f"   â€¢ Tool Call Accuracy: {tool}")
            
            # í‰ê°€ ì´ìœ 
            print("\nï¿½ï¿½ í‰ê°€ ìƒì„¸:")
            
            intent_reason = row.get("outputs.intent_resolution.intent_resolution_reason", "")
            task_reason = row.get("outputs.task_adherence.task_adherence_reason", "")
            tool_reason = row.get("outputs.tool_call_accuracy.tool_call_accuracy_reason", "")
            
            if intent_reason:
                print("\n   [Intent Resolution í‰ê°€ ì´ìœ ]")
                for sentence in intent_reason.split(". "):
                    if sentence.strip():
                        print(f"   â€¢ {sentence.strip()}.")
            
            if task_reason:
                print("\n   [Task Adherence í‰ê°€ ì´ìœ ]")
                for sentence in task_reason.split(". "):
                    if sentence.strip():
                        print(f"   â€¢ {sentence.strip()}.")
            
            if tool_reason:
                print("\n   [Tool Call Accuracy í‰ê°€ ì´ìœ ]")
                for sentence in tool_reason.split(". "):
                    if sentence.strip():
                        print(f"   â€¢ {sentence.strip()}.")
            
            duration = row.get("outputs.operational_metrics.client-run-duration-in-seconds", 0)
            prompt_tokens = row.get("outputs.operational_metrics.prompt-tokens", 0)
            completion_tokens = row.get("outputs.operational_metrics.completion-tokens", 0)
            
            print("\nâ±ï¸  ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
            print(f"   â€¢ ì‹¤í–‰ ì‹œê°„: {duration:.2f}ì´ˆ")
            print(f"   â€¢ í† í° ì‚¬ìš©: {prompt_tokens:,} (ì…ë ¥) + {completion_tokens:,} (ì¶œë ¥) = {prompt_tokens + completion_tokens:,} (ì´)")
            
            issues = []
            if isinstance(intent, (int, float)) and intent < intent_threshold:
                issues.append(f"Intent Resolution ì ìˆ˜ ë‚®ìŒ ({intent:.1f} < {intent_threshold})")
            if isinstance(task, (int, float)) and task < task_threshold:
                issues.append(f"Task Adherence ì ìˆ˜ ë‚®ìŒ ({task:.1f} < {task_threshold})")
            
            if issues:
                print(f"\n{get_score_color(1.0, 3.0)}âš ï¸  ë°œê²¬ëœ ë¬¸ì œ:{reset_color()}")
                for issue in issues:
                    print(f"   â€¢ {issue}")
        
        # ì„¹ì…˜ 4: í†µê³„ ìš”ì•½
        print(f"\n{SEPARATOR}\n")
        print("\nğŸ“ˆ í†µê³„ ìš”ì•½ ë° ë¶„ì„")
        print(LINE)
        
        intent_scores = []
        task_scores = []
        durations = []
        total_tokens_list = []
        failed_queries = []
        
        for idx, row in enumerate(rows, 1):
            intent = row.get("outputs.intent_resolution.intent_resolution")
            task = row.get("outputs.task_adherence.task_adherence")
            duration = row.get("outputs.operational_metrics.client-run-duration-in-seconds", 0)
            prompt = row.get("outputs.operational_metrics.prompt-tokens", 0)
            completion = row.get("outputs.operational_metrics.completion-tokens", 0)
            
            intent_threshold = row.get("outputs.intent_resolution.intent_resolution_threshold", 3)
            task_threshold = row.get("outputs.task_adherence.task_adherence_threshold", 3)
            
            if isinstance(intent, (int, float)):
                intent_scores.append(intent)
                if intent < intent_threshold:
                    query = extract_query_text(row.get("inputs.query", []))
                    failed_queries.append((idx, "Intent Resolution", intent, query[:50]))
            
            if isinstance(task, (int, float)):
                task_scores.append(task)
                if task < task_threshold:
                    query = extract_query_text(row.get("inputs.query", []))
                    failed_queries.append((idx, "Task Adherence", task, query[:50]))
            
            if duration:
                durations.append(duration)
            total_tokens_list.append(prompt + completion)
        
        if intent_scores:
            avg_intent = sum(intent_scores) / len(intent_scores)
            color = get_score_color(avg_intent, 3.0)
            reset = reset_color()
            pass_count = len([s for s in intent_scores if s >= 3.0])
            
            print("\nğŸ“Š Intent Resolution (ì˜ë„ íŒŒì•…)")
            print(f"   í‰ê· : {color}{avg_intent:.2f}/5.0{reset}")
            print(f"   ìµœê³ : {max(intent_scores):.1f}  |  ìµœì €: {min(intent_scores):.1f}")
            print(f"   í•©ê²©ë¥ : {pass_count}/{len(intent_scores)} ({pass_count/len(intent_scores)*100:.1f}%)")
        
        if task_scores:
            avg_task = sum(task_scores) / len(task_scores)
            color = get_score_color(avg_task, 3.0)
            reset = reset_color()
            pass_count = len([s for s in task_scores if s >= 3.0])
            
            print("\nğŸ“Š Task Adherence (ì‘ì—… ì¶©ì‹¤ë„)")
            print(f"   í‰ê· : {color}{avg_task:.2f}/5.0{reset}")
            print(f"   ìµœê³ : {max(task_scores):.1f}  |  ìµœì €: {min(task_scores):.1f}")
            print(f"   í•©ê²©ë¥ : {pass_count}/{len(task_scores)} ({pass_count/len(task_scores)*100:.1f}%)")
        
        if durations:
            print("\nâ±ï¸  ì‹¤í–‰ ì‹œê°„")
            print(f"   í‰ê· : {sum(durations)/len(durations):.2f}ì´ˆ")
            print(f"   ìµœëŒ€: {max(durations):.2f}ì´ˆ  |  ìµœì†Œ: {min(durations):.2f}ì´ˆ")
        
        if total_tokens_list:
            avg_tokens = sum(total_tokens_list) / len(total_tokens_list)
            total_all_tokens = sum(total_tokens_list)
            
            print("\nğŸ’° í† í° ì‚¬ìš©ëŸ‰")
            print(f"   í‰ê· : {avg_tokens:,.0f} tokens/query")
            print(f"   ì´í•©: {total_all_tokens:,} tokens")
            print(f"   ìµœëŒ€: {max(total_tokens_list):,}  |  ìµœì†Œ: {min(total_tokens_list):,}")
            print(f"   ì˜ˆìƒ ë¹„ìš© (GPT-4o): ${(total_all_tokens / 1000) * 0.0025:.4f}")
        
        if failed_queries:
            print(f"\n{get_score_color(1.0, 3.0)}âš ï¸  ê°œì„ ì´ í•„ìš”í•œ ì¿¼ë¦¬ ({len(failed_queries)}ê°œ){reset_color()}")
            print(SEPARATOR)
            
            seen = set()
            for idx, metric, score, query in failed_queries:
                key = (idx, metric)
                if key not in seen:
                    seen.add(key)
                    print(f"   Query #{idx}: {metric} = {score:.1f}")
                    print(f"   â””â”€ {query}...")
                    print()
        else:
            print("\nâœ… ëª¨ë“  ì¿¼ë¦¬ê°€ ì„ê³„ê°’ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")
    
    print(f"\n{LINE}")
    print(f"âœ… ì´ {len(rows)}ê°œ ì¿¼ë¦¬ í‰ê°€ ì™„ë£Œ")
    print(f"ğŸ“ ìƒì„¸ JSON: {eval_output_path.absolute()}")
    print(f"ğŸ’¡ ë…¸íŠ¸ë¶ì—ì„œë„ ë™ì¼í•œ ê²°ê³¼ í™•ì¸ ê°€ëŠ¥ (ì…€ 6)")
    print(f"{LINE}\n")

if __name__ == "__main__":
    main()

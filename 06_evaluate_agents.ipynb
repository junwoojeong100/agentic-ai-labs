{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eea5182",
   "metadata": {},
   "source": [
    "# 06. Agent Evaluation (Agent 평가)\n",
    "\n",
    "이 노트북에서는 배포된 Agent의 품질과 성능을 평가합니다.\n",
    "\n",
    "## 목표\n",
    "\n",
    "- Azure AI Evaluation SDK를 사용한 Agent 평가\n",
    "- 성능, 정확도, 안전성 메트릭 수집\n",
    "- 평가 결과 분석 및 시각화\n",
    "\n",
    "## 사전 요구사항\n",
    "\n",
    "1. Notebook 01-03 완료 (Azure 리소스 및 Agent 배포)\n",
    "2. `config.json` 파일 존재\n",
    "3. Azure AI Project 접근 권한\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784224b9",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 Config 로드\n",
    "\n",
    "이전 노트북에서 생성한 설정을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eebcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# config.json 로드\n",
    "config_path = Path(\"config.json\")\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(\"config.json not found. Please run notebooks 01-03 first.\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# 주요 변수 추출\n",
    "PROJECT_CONNECTION_STRING = config.get(\"project_connection_string\", \"\")\n",
    "simple_project_conn = PROJECT_CONNECTION_STRING.split(';')[0] if PROJECT_CONNECTION_STRING else \"\"\n",
    "\n",
    "print(\"=== Configuration Loaded ===\")\n",
    "print(f\"Resource Group: {config.get('resource_group')}\")\n",
    "print(f\"Location: {config.get('location')}\")\n",
    "print(f\"Project: {simple_project_conn}\")\n",
    "print(f\"Model: {config.get('model_deployment_name')}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc8337",
   "metadata": {},
   "source": [
    "## 2. Azure AI Evaluation 패키지 설치\n",
    "\n",
    "Agent 평가에 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Evaluation 패키지 설치\n",
    "print(\"=== Installing Azure AI Evaluation Package ===\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"azure-ai-evaluation\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ azure-ai-evaluation installed successfully\")\n",
    "else:\n",
    "    print(f\"⚠️  Installation warning: {result.stderr}\")\n",
    "    print(\"   Proceeding anyway...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b155d",
   "metadata": {},
   "source": [
    "## 3. Evaluation 개요\n",
    "\n",
    "### Evaluation 유형\n",
    "\n",
    "**Performance Evaluators (성능 평가):**\n",
    "- **Intent Resolution**: Agent가 사용자 의도를 올바르게 파악했는지 평가\n",
    "- **Tool Call Accuracy**: Agent가 올바른 도구를 정확하게 호출했는지 평가  \n",
    "- **Task Adherence**: Agent가 지시사항을 충실히 따랐는지 평가\n",
    "\n",
    "**Safety Evaluators (안전성 평가):**\n",
    "- **Content Safety**: 부적절한 콘텐츠(폭력, 혐오 등) 포함 여부 평가\n",
    "- **Indirect Attack**: 간접적인 악의적 공격 시도 감지\n",
    "- **Code Vulnerability**: 생성된 코드의 보안 취약점 평가\n",
    "\n",
    "### Evaluation 프로세스\n",
    "\n",
    "1. 테스트 쿼리 생성\n",
    "2. Agent 생성 및 쿼리 실행\n",
    "3. 응답 수집 및 메트릭 측정\n",
    "4. Evaluator로 품질 평가\n",
    "5. 결과 저장 및 분석\n",
    "6. Agent 정리\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4deab",
   "metadata": {},
   "source": [
    "## 4. 테스트 쿼리 생성\n",
    "\n",
    "Agent의 다양한 기능을 테스트할 쿼리를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evals 디렉토리 생성\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "evals_dir = Path(\"evals\")\n",
    "evals_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=== Creating Evaluation Test Queries ===\\n\")\n",
    "\n",
    "# 테스트 쿼리 정의\n",
    "eval_queries = [\n",
    "    {\n",
    "        \"query\": \"안녕하세요. 가족 여행지를 추천받고 싶어요.\",\n",
    "        \"ground-truth\": \"Agent는 인사에 응답하고, 여행지 추천을 위해 Research Agent를 호출해야 합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"부산의 현재 날씨를 알려주세요. 온도와 체감온도를 포함해주세요.\",\n",
    "        \"ground-truth\": \"부산의 현재 날씨 정보를 정확하게 제공해야 하며, 온도와 체감온도를 모두 포함해야 합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"제주도에서 가족과 함께 즐길 수 있는 여행지는 어디인가요?\",\n",
    "        \"ground-truth\": \"제주도의 가족 친화적인 여행지를 검색하여 추천해야 합니다. 자연 명소, 체험 활동, 접근성 등을 고려한 정보를 제공합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"서핑할 수 있는 해변을 추천해주세요.\",\n",
    "        \"ground-truth\": \"서핑이 가능한 한국의 해변 여행지를 검색하여 추천해야 합니다. 양양, 부산 등의 서핑 명소 정보를 제공합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"가을에 가기 좋은 단풍 명소는 어디인가요?\",\n",
    "        \"ground-truth\": \"가을 계절에 방문하기 좋은 단풍 명소를 검색하여 추천해야 합니다. 내장산, 설악산 등의 자연 명소 정보를 제공합니다.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# JSON 파일로 저장\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "with open(eval_queries_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Created {eval_queries_path}\")\n",
    "print(f\"\\n📋 Test Queries ({len(eval_queries)} total):\\n\")\n",
    "\n",
    "for i, query in enumerate(eval_queries, 1):\n",
    "    print(f\"   {i}. {query['query'][:60]}...\")\n",
    "\n",
    "print(\"\\n💡 각 쿼리는 Agent의 다른 기능을 테스트합니다:\")\n",
    "print(\"   • 일반 대화 및 여행 의도 파악\")\n",
    "print(\"   • 날씨 조회 (Tool 기능)\")\n",
    "print(\"   • 여행지 지식 검색 (RAG 기능)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded78b6",
   "metadata": {},
   "source": [
    "## 5. Agent Evaluation 실행\n",
    "\n",
    "평가용 Agent를 생성하고 테스트 쿼리를 실행한 후, Evaluator로 품질을 평가합니다.\n",
    "\n",
    "### 참고사항\n",
    "\n",
    "- **실행 시간**: 약 2-3분 소요\n",
    "- **리전 제약**: eastus 리전에서는 일부 Safety Evaluator가 지원되지 않습니다\n",
    "- **권한 이슈**: Azure AI Project의 스토리지 권한이 필요할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Evaluation 실행\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from azure.ai.agents.models import RunStatus, MessageRole\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.evaluation import (\n",
    "    AIAgentConverter, evaluate, ToolCallAccuracyEvaluator, IntentResolutionEvaluator, \n",
    "    TaskAdherenceEvaluator\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"=== Running Agent Evaluation ===\\n\")\n",
    "\n",
    "# 파일 경로 설정\n",
    "current_dir = Path(\".\")\n",
    "evals_dir = current_dir / \"evals\"\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "eval_input_path = evals_dir / \"eval-input.jsonl\"\n",
    "eval_output_path = evals_dir / \"eval-output.json\"\n",
    "\n",
    "# 환경 변수 로드 (이미 config에서 로드됨)\n",
    "project_endpoint = simple_project_conn\n",
    "parsed_endpoint = urlparse(project_endpoint)\n",
    "model_endpoint = f\"{parsed_endpoint.scheme}://{parsed_endpoint.netloc}\"\n",
    "deployment_name = config.get(\"model_deployment_name\", \"gpt-4o\")\n",
    "\n",
    "print(f\"📋 Configuration:\")\n",
    "print(f\"   Project: {project_endpoint}\")\n",
    "print(f\"   Model: {deployment_name}\")\n",
    "print(f\"   Test Queries: {eval_queries_path}\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Initialize AIProjectClient\n",
    "print(\"🔌 Connecting to AI Project...\")\n",
    "credential = DefaultAzureCredential()\n",
    "ai_project = AIProjectClient(\n",
    "    credential=credential,\n",
    "    endpoint=project_endpoint,\n",
    "    api_version=\"2025-05-15-preview\"  # Evaluations require preview API\n",
    ")\n",
    "print(\"✅ Connected\\n\")\n",
    "\n",
    "# Evaluation용 Agent 생성\n",
    "print(\"🤖 Creating Evaluation Agent...\")\n",
    "eval_agent = ai_project.agents.create_agent(\n",
    "    model=deployment_name,\n",
    "    name=\"Evaluation Agent\",\n",
    "    instructions=\"\"\"You are a helpful travel and weather assistant.\n",
    "    \n",
    "You can help users with:\n",
    "1. Travel recommendations and destination information\n",
    "2. Weather information for any city\n",
    "3. General travel planning advice\n",
    "\n",
    "Be friendly, informative, and provide detailed responses.\"\"\"\n",
    ")\n",
    "print(f\"✅ Agent created: {eval_agent.name} (ID: {eval_agent.id})\\n\")\n",
    "\n",
    "# Setup evaluation config\n",
    "api_version = config.get(\"api_version\", \"2024-08-01-preview\")\n",
    "model_config = {\n",
    "    \"azure_deployment\": deployment_name,\n",
    "    \"azure_endpoint\": model_endpoint,\n",
    "    \"api_version\": api_version,\n",
    "}\n",
    "\n",
    "thread_data_converter = AIAgentConverter(ai_project)\n",
    "\n",
    "# 테스트 쿼리 실행 및 evaluation input 준비\n",
    "print(\"=\"*70)\n",
    "print(\"📝 Executing Test Queries\\n\")\n",
    "\n",
    "with open(eval_queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"   Total queries: {len(test_data)}\\n\")\n",
    "\n",
    "with open(eval_input_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, row in enumerate(test_data, 1):\n",
    "        query_text = row.get(\"query\")\n",
    "        print(f\"   [{idx}/{len(test_data)}] {query_text[:50]}...\")\n",
    "        \n",
    "        # 새 스레드 생성 (각 쿼리를 격리)\n",
    "        thread = ai_project.agents.threads.create()\n",
    "        \n",
    "        # 사용자 쿼리 생성\n",
    "        ai_project.agents.messages.create(\n",
    "            thread.id, role=MessageRole.USER, content=query_text\n",
    "        )\n",
    "        \n",
    "        # Agent 실행 및 성능 측정\n",
    "        start_time = time.time()\n",
    "        run = ai_project.agents.runs.create_and_process(\n",
    "            thread_id=thread.id, agent_id=eval_agent.id\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if run.status != RunStatus.COMPLETED:\n",
    "            print(f\"      ⚠️  Run failed: {run.last_error or 'Unknown error'}\")\n",
    "            continue\n",
    "        \n",
    "        # 운영 메트릭 수집\n",
    "        operational_metrics = {\n",
    "            \"server-run-duration-in-seconds\": (\n",
    "                run.completed_at - run.created_at\n",
    "            ).total_seconds(),\n",
    "            \"client-run-duration-in-seconds\": end_time - start_time,\n",
    "            \"completion-tokens\": run.usage.completion_tokens,\n",
    "            \"prompt-tokens\": run.usage.prompt_tokens,\n",
    "            \"ground-truth\": row.get(\"ground-truth\", '')\n",
    "        }\n",
    "        \n",
    "        # Thread 데이터 + 운영 메트릭을 evaluation input에 추가\n",
    "        evaluation_data = thread_data_converter.prepare_evaluation_data(thread_ids=thread.id)\n",
    "        eval_item = evaluation_data[0]\n",
    "        eval_item[\"metrics\"] = operational_metrics\n",
    "        f.write(json.dumps(eval_item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"      ✅ Completed in {operational_metrics['client-run-duration-in-seconds']:.1f}s\")\n",
    "        print(f\"         Tokens: {operational_metrics['prompt-tokens']} prompt + {operational_metrics['completion-tokens']} completion\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ All queries executed successfully\")\n",
    "print(f\"   Input saved to: {eval_input_path}\\n\")\n",
    "\n",
    "# Evaluation 실행\n",
    "print(\"=\"*70)\n",
    "print(\"🔬 Running Evaluators\\n\")\n",
    "\n",
    "print(\"   Evaluators:\")\n",
    "print(\"      • ToolCallAccuracyEvaluator\")\n",
    "print(\"      • IntentResolutionEvaluator\")\n",
    "print(\"      • TaskAdherenceEvaluator\")\n",
    "print(\"\\n   ⚠️  참고: eastus 리전에서는 일부 RAI 평가자가 지원되지 않습니다.\")\n",
    "print(\"      (CodeVulnerability, ContentSafety, IndirectAttack는 제외)\\n\")\n",
    "print(\"   ⏳ This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# OperationalMetricsEvaluator 정의\n",
    "class OperationalMetricsEvaluator:\n",
    "    \"\"\"Propagate operational metrics to the final evaluation results\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, *, metrics: dict, **kwargs):\n",
    "        return metrics\n",
    "\n",
    "# Evaluation 실행 (로컬)\n",
    "print(\"   💡 평가 결과를 로컬에 저장합니다.\\n\")\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"foundry-agent-evaluation\",\n",
    "    data=eval_input_path,\n",
    "    evaluators={\n",
    "        \"operational_metrics\": OperationalMetricsEvaluator(),\n",
    "        \"tool_call_accuracy\": ToolCallAccuracyEvaluator(model_config=model_config),\n",
    "        \"intent_resolution\": IntentResolutionEvaluator(model_config=model_config),\n",
    "        \"task_adherence\": TaskAdherenceEvaluator(model_config=model_config),\n",
    "        # eastus 리전에서 지원되지 않는 평가자들은 제외\n",
    "        # \"code_vulnerability\": CodeVulnerabilityEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"content_safety\": ContentSafetyEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"indirect_attack\": IndirectAttackEvaluator(credential=credential, azure_ai_project=project_endpoint)\n",
    "    },\n",
    "    output_path=eval_output_path,\n",
    "    # azure_ai_project 파라미터 제거 (ML workspace 불필요)\n",
    ")\n",
    "\n",
    "print(\"✅ Evaluation completed!\\n\")\n",
    "print(f\"📁 결과 저장 위치: {eval_output_path}\")\n",
    "print(f\"   다음 셀에서 결과를 확인하세요.\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluation Agent 삭제\n",
    "print(\"🧹 Cleaning up...\")\n",
    "ai_project.agents.delete_agent(eval_agent.id)\n",
    "print(f\"✅ Evaluation Agent deleted: {eval_agent.id}\\n\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dbb91",
   "metadata": {},
   "source": [
    "## 6. Evaluation 결과 시각화\n",
    "\n",
    "결과를 표와 차트로 시각화하여 분석합니다.\n",
    "\n",
    "### 결과 확인 방법\n",
    "\n",
    "**방법 1: 노트북에서 확인**\n",
    "- 아래 셀을 실행하여 노트북에서 바로 결과를 확인합니다.\n",
    "\n",
    "**방법 2: 터미널에서 확인**\n",
    "- 터미널에서 다음 명령어를 실행하여 동일한 결과를 확인할 수 있습니다:\n",
    "  ```bash\n",
    "  python3 show_eval_results.py\n",
    "  ```\n",
    "- 이 스크립트는 프로젝트 루트에 자동으로 생성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation 결과 상세 시각화\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "eval_output_path = Path(\"evals/eval-output.json\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"📊 AGENT EVALUATION RESULTS (상세)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "if not eval_output_path.exists():\n",
    "    print(\"❌ 평가 결과 파일이 없습니다. 먼저 셀 5를 실행하세요.\\n\")\n",
    "else:\n",
    "    with open(eval_output_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metrics = data.get('metrics', {})\n",
    "    rows = data.get('rows', [])\n",
    "    \n",
    "    # 1. 전체 평균 점수\n",
    "    print(\"⭐ 전체 평균 성능 점수\")\n",
    "    print(\"-\" * 80)\n",
    "    scores = [\n",
    "        ('Intent Resolution', 'intent_resolution.intent_resolution', '의도 파악'),\n",
    "        ('Task Adherence', 'task_adherence.task_adherence', '작업 충실도'),\n",
    "    ]\n",
    "    \n",
    "    for name, key, desc in scores:\n",
    "        if key in metrics:\n",
    "            score = metrics[key]\n",
    "            stars = '★' * int(score) + '☆' * (5 - int(score))\n",
    "            bar = '█' * int(score * 4) + '░' * (20 - int(score * 4))\n",
    "            print(f\"  {name:20} {score:.2f}/5.0  {stars}  [{bar}]\")\n",
    "            print(f\"  {'':20} → {desc}\")\n",
    "    \n",
    "    # 2. 운영 메트릭\n",
    "    print(\"\\n\\n⚡ 운영 메트릭 (평균)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    operational_keys = [\n",
    "        ('operational_metrics.server-run-duration-in-seconds', '서버 실행 시간', 's'),\n",
    "        ('operational_metrics.client-run-duration-in-seconds', '클라이언트 실행 시간', 's'),\n",
    "        ('operational_metrics.prompt-tokens', '프롬프트 토큰', 'tokens'),\n",
    "        ('operational_metrics.completion-tokens', '완성 토큰', 'tokens'),\n",
    "    ]\n",
    "    \n",
    "    for key, desc, unit in operational_keys:\n",
    "        if key in metrics:\n",
    "            value = metrics[key]\n",
    "            if unit == 'tokens':\n",
    "                print(f\"  {desc:25} {int(value):>8,} {unit}\")\n",
    "            else:\n",
    "                print(f\"  {desc:25} {value:>8.2f} {unit}\")\n",
    "    \n",
    "    # 3. 개별 쿼리 결과\n",
    "    if rows:\n",
    "        print(\"\\n\\n📋 쿼리별 상세 결과\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            # 쿼리 텍스트 추출\n",
    "            query = \"\"\n",
    "            query_input = row.get('inputs.query', [])\n",
    "            if isinstance(query_input, list):\n",
    "                for item in query_input:\n",
    "                    if isinstance(item, dict) and item.get('role') == 'user':\n",
    "                        content = item.get('content', [])\n",
    "                        if isinstance(content, list) and len(content) > 0:\n",
    "                            query = content[0].get('text', '')\n",
    "                        break\n",
    "            \n",
    "            print(f\"\\n[Query {idx}]\")\n",
    "            print(f\"질문: {query[:100]}{'...' if len(query) > 100 else ''}\")\n",
    "            \n",
    "            # 점수\n",
    "            intent = row.get('outputs.intent_resolution.intent_resolution', 'N/A')\n",
    "            task = row.get('outputs.task_adherence.task_adherence', 'N/A')\n",
    "            tool = row.get('outputs.tool_call_accuracy.tool_call_accuracy', 'N/A')\n",
    "            \n",
    "            print(f\"  Intent Resolution:  {intent if isinstance(intent, str) else f'{intent:.1f}/5.0'}\")\n",
    "            print(f\"  Task Adherence:     {task if isinstance(task, str) else f'{task:.1f}/5.0'}\")\n",
    "            print(f\"  Tool Call Accuracy: {tool}\")\n",
    "            \n",
    "            # 운영 메트릭\n",
    "            duration = row.get('outputs.operational_metrics.client-run-duration-in-seconds', 0)\n",
    "            prompt_tokens = row.get('outputs.operational_metrics.prompt-tokens', 0)\n",
    "            completion_tokens = row.get('outputs.operational_metrics.completion-tokens', 0)\n",
    "            \n",
    "            print(f\"  실행 시간: {duration:.2f}s  |  토큰: {prompt_tokens} + {completion_tokens} = {prompt_tokens + completion_tokens}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # 4. 통계 요약\n",
    "        print(\"\\n\\n📈 통계 요약\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        intent_scores = []\n",
    "        task_scores = []\n",
    "        durations = []\n",
    "        total_tokens = []\n",
    "        \n",
    "        for row in rows:\n",
    "            intent = row.get('outputs.intent_resolution.intent_resolution')\n",
    "            task = row.get('outputs.task_adherence.task_adherence')\n",
    "            duration = row.get('outputs.operational_metrics.client-run-duration-in-seconds', 0)\n",
    "            prompt = row.get('outputs.operational_metrics.prompt-tokens', 0)\n",
    "            completion = row.get('outputs.operational_metrics.completion-tokens', 0)\n",
    "            \n",
    "            if isinstance(intent, (int, float)):\n",
    "                intent_scores.append(intent)\n",
    "            if isinstance(task, (int, float)):\n",
    "                task_scores.append(task)\n",
    "            if duration:\n",
    "                durations.append(duration)\n",
    "            total_tokens.append(prompt + completion)\n",
    "        \n",
    "        if intent_scores:\n",
    "            print(f\"Intent Resolution 평균:  {sum(intent_scores)/len(intent_scores):.2f}/5.0\")\n",
    "            print(f\"  최고: {max(intent_scores):.1f}  |  최저: {min(intent_scores):.1f}\")\n",
    "        \n",
    "        if task_scores:\n",
    "            print(f\"\\nTask Adherence 평균:     {sum(task_scores)/len(task_scores):.2f}/5.0\")\n",
    "            print(f\"  최고: {max(task_scores):.1f}  |  최저: {min(task_scores):.1f}\")\n",
    "        \n",
    "        if durations:\n",
    "            print(f\"\\n실행 시간 평균:           {sum(durations)/len(durations):.2f}s\")\n",
    "            print(f\"  최고: {max(durations):.2f}s  |  최저: {min(durations):.2f}s\")\n",
    "        \n",
    "        if total_tokens:\n",
    "            print(f\"\\n토큰 사용 평균:           {sum(total_tokens)/len(total_tokens):.0f} tokens\")\n",
    "            print(f\"  최대: {max(total_tokens)}  |  최소: {min(total_tokens)}\")\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(f\"✅ 총 {len(rows)}개 쿼리 평가 완료\")\n",
    "    print(f\"📁 상세 JSON: {eval_output_path}\")\n",
    "    print(f\"💡 터미널에서 실행: python3 show_eval_results.py\")\n",
    "    print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4900276",
   "metadata": {},
   "source": [
    "## 7. Evaluation 메트릭 해석 가이드\n",
    "\n",
    "### 메트릭 해석\n",
    "\n",
    "**Operational Metrics:**\n",
    "- `server-run-duration-in-seconds`: 서버에서 Agent 실행 시간\n",
    "- `client-run-duration-in-seconds`: 클라이언트에서 측정한 총 소요 시간\n",
    "- `prompt-tokens`: 입력 토큰 수\n",
    "- `completion-tokens`: 생성된 토큰 수\n",
    "\n",
    "**Performance Metrics:**\n",
    "- `tool_call_accuracy.*`: 도구 호출 정확도 (1-5점)\n",
    "- `intent_resolution.*`: 의도 파악 정확도 (1-5점)\n",
    "- `task_adherence.*`: 작업 준수도 (1-5점)\n",
    "\n",
    "**점수 해석:**\n",
    "- 5점: 완벽함\n",
    "- 4점: 좋음\n",
    "- 3점: 보통\n",
    "- 2점: 개선 필요\n",
    "- 1점: 매우 나쁨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1fd41",
   "metadata": {},
   "source": [
    "## 8. 다음 단계 및 Agent 개선\n",
    "\n",
    "### Agent 개선 방법\n",
    "\n",
    "1. **낮은 점수 분석**\n",
    "   - 어떤 쿼리에서 점수가 낮았는지 확인\n",
    "   - 어떤 evaluator에서 문제가 발생했는지 파악\n",
    "\n",
    "2. **프롬프트 개선**\n",
    "   - Agent instructions를 더 명확하게 작성\n",
    "   - 예시 추가\n",
    "   - 제약사항 명시\n",
    "\n",
    "3. **기능 개선**\n",
    "   - Tool 추가 또는 개선\n",
    "   - RAG 지식 베이스 보강\n",
    "   - Multi-Agent 조정\n",
    "\n",
    "4. **재평가**\n",
    "   - 동일한 쿼리로 재평가\n",
    "   - 점수 변화 비교\n",
    "   - 지속적인 개선\n",
    "\n",
    "### 참고 자료\n",
    "\n",
    "- [Azure AI Foundry Agent Evaluation](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)\n",
    "- [Built-in Evaluators](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk)\n",
    "- [Evaluation Best Practices](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-approach-gen-ai)\n",
    "\n",
    "---\n",
    "\n",
    "## 완료!\n",
    "\n",
    "축하합니다! Agent Evaluation을 성공적으로 완료했습니다. 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

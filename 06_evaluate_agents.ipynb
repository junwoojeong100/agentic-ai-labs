{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eea5182",
   "metadata": {},
   "source": [
    "# 06. Agent Evaluation (Agent í‰ê°€)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë°°í¬ëœ Agentì˜ í’ˆì§ˆê³¼ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "\n",
    "- Azure AI Evaluation SDKë¥¼ ì‚¬ìš©í•œ Agent í‰ê°€\n",
    "- ì„±ëŠ¥, ì •í™•ë„, ì•ˆì „ì„± ë©”íŠ¸ë¦­ ìˆ˜ì§‘\n",
    "- í‰ê°€ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
    "\n",
    "## ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
    "\n",
    "1. Notebook 01-03 ì™„ë£Œ (Azure ë¦¬ì†ŒìŠ¤ ë° Agent ë°°í¬)\n",
    "2. `config.json` íŒŒì¼ ì¡´ì¬\n",
    "3. Azure AI Project ì ‘ê·¼ ê¶Œí•œ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784224b9",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° Config ë¡œë“œ\n",
    "\n",
    "ì´ì „ ë…¸íŠ¸ë¶ì—ì„œ ìƒì„±í•œ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eebcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# config.json ë¡œë“œ\n",
    "config_path = Path(\"config.json\")\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(\"config.json not found. Please run notebooks 01-03 first.\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# ì£¼ìš” ë³€ìˆ˜ ì¶”ì¶œ\n",
    "PROJECT_CONNECTION_STRING = config.get(\"project_connection_string\", \"\")\n",
    "simple_project_conn = PROJECT_CONNECTION_STRING.split(';')[0] if PROJECT_CONNECTION_STRING else \"\"\n",
    "\n",
    "print(\"=== Configuration Loaded ===\")\n",
    "print(f\"Resource Group: {config.get('resource_group')}\")\n",
    "print(f\"Location: {config.get('location')}\")\n",
    "print(f\"Project: {simple_project_conn}\")\n",
    "print(f\"Model: {config.get('model_deployment_name')}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc8337",
   "metadata": {},
   "source": [
    "## 2. Azure AI Evaluation íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "Agent í‰ê°€ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Evaluation íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "print(\"=== Installing Azure AI Evaluation Package ===\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"azure-ai-evaluation\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"âœ… azure-ai-evaluation installed successfully\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Installation warning: {result.stderr}\")\n",
    "    print(\"   Proceeding anyway...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b155d",
   "metadata": {},
   "source": [
    "## 3. Evaluation ê°œìš”\n",
    "\n",
    "### Evaluation ìœ í˜•\n",
    "\n",
    "**Performance Evaluators (ì„±ëŠ¥ í‰ê°€):**\n",
    "- **Intent Resolution**: Agentê°€ ì‚¬ìš©ì ì˜ë„ë¥¼ ì˜¬ë°”ë¥´ê²Œ íŒŒì•…í–ˆëŠ”ì§€ í‰ê°€\n",
    "- **Tool Call Accuracy**: Agentê°€ ì˜¬ë°”ë¥¸ ë„êµ¬ë¥¼ ì •í™•í•˜ê²Œ í˜¸ì¶œí–ˆëŠ”ì§€ í‰ê°€  \n",
    "- **Task Adherence**: Agentê°€ ì§€ì‹œì‚¬í•­ì„ ì¶©ì‹¤íˆ ë”°ëëŠ”ì§€ í‰ê°€\n",
    "\n",
    "**Safety Evaluators (ì•ˆì „ì„± í‰ê°€):**\n",
    "- **Content Safety**: ë¶€ì ì ˆí•œ ì½˜í…ì¸ (í­ë ¥, í˜ì˜¤ ë“±) í¬í•¨ ì—¬ë¶€ í‰ê°€\n",
    "- **Indirect Attack**: ê°„ì ‘ì ì¸ ì•…ì˜ì  ê³µê²© ì‹œë„ ê°ì§€\n",
    "- **Code Vulnerability**: ìƒì„±ëœ ì½”ë“œì˜ ë³´ì•ˆ ì·¨ì•½ì  í‰ê°€\n",
    "\n",
    "### Evaluation í”„ë¡œì„¸ìŠ¤\n",
    "\n",
    "1. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±\n",
    "2. Agent ìƒì„± ë° ì¿¼ë¦¬ ì‹¤í–‰\n",
    "3. ì‘ë‹µ ìˆ˜ì§‘ ë° ë©”íŠ¸ë¦­ ì¸¡ì •\n",
    "4. Evaluatorë¡œ í’ˆì§ˆ í‰ê°€\n",
    "5. ê²°ê³¼ ì €ì¥ ë° ë¶„ì„\n",
    "6. Agent ì •ë¦¬\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4deab",
   "metadata": {},
   "source": [
    "## 4. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±\n",
    "\n",
    "Agentì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evals ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "evals_dir = Path(\"evals\")\n",
    "evals_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=== Creating Evaluation Test Queries ===\\n\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜\n",
    "eval_queries = [\n",
    "    {\n",
    "        \"query\": \"ì•ˆë…•í•˜ì„¸ìš”. ê°€ì¡± ì—¬í–‰ì§€ë¥¼ ì¶”ì²œë°›ê³  ì‹¶ì–´ìš”.\",\n",
    "        \"ground-truth\": \"AgentëŠ” ì¸ì‚¬ì— ì‘ë‹µí•˜ê³ , ì—¬í–‰ì§€ ì¶”ì²œì„ ìœ„í•´ Research Agentë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ë¶€ì‚°ì˜ í˜„ì¬ ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ì˜¨ë„ì™€ ì²´ê°ì˜¨ë„ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.\",\n",
    "        \"ground-truth\": \"ë¶€ì‚°ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì œê³µí•´ì•¼ í•˜ë©°, ì˜¨ë„ì™€ ì²´ê°ì˜¨ë„ë¥¼ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ì œì£¼ë„ì—ì„œ ê°€ì¡±ê³¼ í•¨ê»˜ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì—¬í–‰ì§€ëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
    "        \"ground-truth\": \"ì œì£¼ë„ì˜ ê°€ì¡± ì¹œí™”ì ì¸ ì—¬í–‰ì§€ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤. ìì—° ëª…ì†Œ, ì²´í—˜ í™œë™, ì ‘ê·¼ì„± ë“±ì„ ê³ ë ¤í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ì„œí•‘í•  ìˆ˜ ìˆëŠ” í•´ë³€ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.\",\n",
    "        \"ground-truth\": \"ì„œí•‘ì´ ê°€ëŠ¥í•œ í•œêµ­ì˜ í•´ë³€ ì—¬í–‰ì§€ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤. ì–‘ì–‘, ë¶€ì‚° ë“±ì˜ ì„œí•‘ ëª…ì†Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ê°€ì„ì— ê°€ê¸° ì¢‹ì€ ë‹¨í’ ëª…ì†ŒëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
    "        \"ground-truth\": \"ê°€ì„ ê³„ì ˆì— ë°©ë¬¸í•˜ê¸° ì¢‹ì€ ë‹¨í’ ëª…ì†Œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤. ë‚´ì¥ì‚°, ì„¤ì•…ì‚° ë“±ì˜ ìì—° ëª…ì†Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "with open(eval_queries_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Created {eval_queries_path}\")\n",
    "print(f\"\\nğŸ“‹ Test Queries ({len(eval_queries)} total):\\n\")\n",
    "\n",
    "for i, query in enumerate(eval_queries, 1):\n",
    "    print(f\"   {i}. {query['query'][:60]}...\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ê° ì¿¼ë¦¬ëŠ” Agentì˜ ë‹¤ë¥¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:\")\n",
    "print(\"   â€¢ ì¼ë°˜ ëŒ€í™” ë° ì—¬í–‰ ì˜ë„ íŒŒì•…\")\n",
    "print(\"   â€¢ ë‚ ì”¨ ì¡°íšŒ (Tool ê¸°ëŠ¥)\")\n",
    "print(\"   â€¢ ì—¬í–‰ì§€ ì§€ì‹ ê²€ìƒ‰ (RAG ê¸°ëŠ¥)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded78b6",
   "metadata": {},
   "source": [
    "## 5. Agent Evaluation ì‹¤í–‰\n",
    "\n",
    "í‰ê°€ìš© Agentë¥¼ ìƒì„±í•˜ê³  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•œ í›„, Evaluatorë¡œ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì°¸ê³ ì‚¬í•­\n",
    "\n",
    "- **ì‹¤í–‰ ì‹œê°„**: ì•½ 2-3ë¶„ ì†Œìš”\n",
    "- **ë¦¬ì „ ì œì•½**: eastus ë¦¬ì „ì—ì„œëŠ” ì¼ë¶€ Safety Evaluatorê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\n",
    "- **ê¶Œí•œ ì´ìŠˆ**: Azure AI Projectì˜ ìŠ¤í† ë¦¬ì§€ ê¶Œí•œì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Evaluation ì‹¤í–‰\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from azure.ai.agents.models import RunStatus, MessageRole\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.evaluation import (\n",
    "    AIAgentConverter, evaluate, ToolCallAccuracyEvaluator, IntentResolutionEvaluator, \n",
    "    TaskAdherenceEvaluator\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"=== Running Agent Evaluation ===\\n\")\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "current_dir = Path(\".\")\n",
    "evals_dir = current_dir / \"evals\"\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "eval_input_path = evals_dir / \"eval-input.jsonl\"\n",
    "eval_output_path = evals_dir / \"eval-output.json\"\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ì´ë¯¸ configì—ì„œ ë¡œë“œë¨)\n",
    "project_endpoint = simple_project_conn\n",
    "parsed_endpoint = urlparse(project_endpoint)\n",
    "model_endpoint = f\"{parsed_endpoint.scheme}://{parsed_endpoint.netloc}\"\n",
    "deployment_name = config.get(\"model_deployment_name\", \"gpt-4o\")\n",
    "\n",
    "print(f\"ğŸ“‹ Configuration:\")\n",
    "print(f\"   Project: {project_endpoint}\")\n",
    "print(f\"   Model: {deployment_name}\")\n",
    "print(f\"   Test Queries: {eval_queries_path}\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Initialize AIProjectClient\n",
    "print(\"ğŸ”Œ Connecting to AI Project...\")\n",
    "credential = DefaultAzureCredential()\n",
    "ai_project = AIProjectClient(\n",
    "    credential=credential,\n",
    "    endpoint=project_endpoint,\n",
    "    api_version=\"2025-05-15-preview\"  # Evaluations require preview API\n",
    ")\n",
    "print(\"âœ… Connected\\n\")\n",
    "\n",
    "# Evaluationìš© Agent ìƒì„±\n",
    "print(\"ğŸ¤– Creating Evaluation Agent...\")\n",
    "eval_agent = ai_project.agents.create_agent(\n",
    "    model=deployment_name,\n",
    "    name=\"Evaluation Agent\",\n",
    "    instructions=\"\"\"You are a helpful travel and weather assistant.\n",
    "    \n",
    "You can help users with:\n",
    "1. Travel recommendations and destination information\n",
    "2. Weather information for any city\n",
    "3. General travel planning advice\n",
    "\n",
    "Be friendly, informative, and provide detailed responses.\"\"\"\n",
    ")\n",
    "print(f\"âœ… Agent created: {eval_agent.name} (ID: {eval_agent.id})\\n\")\n",
    "\n",
    "# Setup evaluation config\n",
    "api_version = config.get(\"api_version\", \"2024-08-01-preview\")\n",
    "model_config = {\n",
    "    \"azure_deployment\": deployment_name,\n",
    "    \"azure_endpoint\": model_endpoint,\n",
    "    \"api_version\": api_version,\n",
    "}\n",
    "\n",
    "thread_data_converter = AIAgentConverter(ai_project)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰ ë° evaluation input ì¤€ë¹„\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“ Executing Test Queries\\n\")\n",
    "\n",
    "with open(eval_queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"   Total queries: {len(test_data)}\\n\")\n",
    "\n",
    "with open(eval_input_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, row in enumerate(test_data, 1):\n",
    "        query_text = row.get(\"query\")\n",
    "        print(f\"   [{idx}/{len(test_data)}] {query_text[:50]}...\")\n",
    "        \n",
    "        # ìƒˆ ìŠ¤ë ˆë“œ ìƒì„± (ê° ì¿¼ë¦¬ë¥¼ ê²©ë¦¬)\n",
    "        thread = ai_project.agents.threads.create()\n",
    "        \n",
    "        # ì‚¬ìš©ì ì¿¼ë¦¬ ìƒì„±\n",
    "        ai_project.agents.messages.create(\n",
    "            thread.id, role=MessageRole.USER, content=query_text\n",
    "        )\n",
    "        \n",
    "        # Agent ì‹¤í–‰ ë° ì„±ëŠ¥ ì¸¡ì •\n",
    "        start_time = time.time()\n",
    "        run = ai_project.agents.runs.create_and_process(\n",
    "            thread_id=thread.id, agent_id=eval_agent.id\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if run.status != RunStatus.COMPLETED:\n",
    "            print(f\"      âš ï¸  Run failed: {run.last_error or 'Unknown error'}\")\n",
    "            continue\n",
    "        \n",
    "        # ìš´ì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘\n",
    "        operational_metrics = {\n",
    "            \"server-run-duration-in-seconds\": (\n",
    "                run.completed_at - run.created_at\n",
    "            ).total_seconds(),\n",
    "            \"client-run-duration-in-seconds\": end_time - start_time,\n",
    "            \"completion-tokens\": run.usage.completion_tokens,\n",
    "            \"prompt-tokens\": run.usage.prompt_tokens,\n",
    "            \"ground-truth\": row.get(\"ground-truth\", '')\n",
    "        }\n",
    "        \n",
    "        # Thread ë°ì´í„° + ìš´ì˜ ë©”íŠ¸ë¦­ì„ evaluation inputì— ì¶”ê°€\n",
    "        evaluation_data = thread_data_converter.prepare_evaluation_data(thread_ids=thread.id)\n",
    "        eval_item = evaluation_data[0]\n",
    "        eval_item[\"metrics\"] = operational_metrics\n",
    "        f.write(json.dumps(eval_item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"      âœ… Completed in {operational_metrics['client-run-duration-in-seconds']:.1f}s\")\n",
    "        print(f\"         Tokens: {operational_metrics['prompt-tokens']} prompt + {operational_metrics['completion-tokens']} completion\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ… All queries executed successfully\")\n",
    "print(f\"   Input saved to: {eval_input_path}\\n\")\n",
    "\n",
    "# Evaluation ì‹¤í–‰\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ”¬ Running Evaluators\\n\")\n",
    "\n",
    "print(\"   Evaluators:\")\n",
    "print(\"      â€¢ ToolCallAccuracyEvaluator\")\n",
    "print(\"      â€¢ IntentResolutionEvaluator\")\n",
    "print(\"      â€¢ TaskAdherenceEvaluator\")\n",
    "print(\"\\n   âš ï¸  ì°¸ê³ : eastus ë¦¬ì „ì—ì„œëŠ” ì¼ë¶€ RAI í‰ê°€ìê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "print(\"      (CodeVulnerability, ContentSafety, IndirectAttackëŠ” ì œì™¸)\\n\")\n",
    "print(\"   â³ This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# OperationalMetricsEvaluator ì •ì˜\n",
    "class OperationalMetricsEvaluator:\n",
    "    \"\"\"Propagate operational metrics to the final evaluation results\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, *, metrics: dict, **kwargs):\n",
    "        return metrics\n",
    "\n",
    "# Evaluation ì‹¤í–‰ (ë¡œì»¬)\n",
    "print(\"   ğŸ’¡ í‰ê°€ ê²°ê³¼ë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"foundry-agent-evaluation\",\n",
    "    data=eval_input_path,\n",
    "    evaluators={\n",
    "        \"operational_metrics\": OperationalMetricsEvaluator(),\n",
    "        \"tool_call_accuracy\": ToolCallAccuracyEvaluator(model_config=model_config),\n",
    "        \"intent_resolution\": IntentResolutionEvaluator(model_config=model_config),\n",
    "        \"task_adherence\": TaskAdherenceEvaluator(model_config=model_config),\n",
    "        # eastus ë¦¬ì „ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠëŠ” í‰ê°€ìë“¤ì€ ì œì™¸\n",
    "        # \"code_vulnerability\": CodeVulnerabilityEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"content_safety\": ContentSafetyEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"indirect_attack\": IndirectAttackEvaluator(credential=credential, azure_ai_project=project_endpoint)\n",
    "    },\n",
    "    output_path=eval_output_path,\n",
    "    # azure_ai_project íŒŒë¼ë¯¸í„° ì œê±° (ML workspace ë¶ˆí•„ìš”)\n",
    ")\n",
    "\n",
    "print(\"âœ… Evaluation completed!\\n\")\n",
    "print(f\"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {eval_output_path}\")\n",
    "print(f\"   ë‹¤ìŒ ì…€ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluation Agent ì‚­ì œ\n",
    "print(\"ğŸ§¹ Cleaning up...\")\n",
    "ai_project.agents.delete_agent(eval_agent.id)\n",
    "print(f\"âœ… Evaluation Agent deleted: {eval_agent.id}\\n\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dbb91",
   "metadata": {},
   "source": [
    "## 6. Evaluation ê²°ê³¼ ì‹œê°í™”\n",
    "\n",
    "ê²°ê³¼ë¥¼ í‘œì™€ ì°¨íŠ¸ë¡œ ì‹œê°í™”í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ê²°ê³¼ í™•ì¸ ë°©ë²•\n",
    "\n",
    "**ë°©ë²• 1: ë…¸íŠ¸ë¶ì—ì„œ í™•ì¸**\n",
    "- ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ë…¸íŠ¸ë¶ì—ì„œ ë°”ë¡œ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë°©ë²• 2: í„°ë¯¸ë„ì—ì„œ í™•ì¸**\n",
    "- í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ë™ì¼í•œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "  ```bash\n",
    "  python3 show_eval_results.py\n",
    "  ```\n",
    "- ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìë™ìœ¼ë¡œ ìƒì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation ê²°ê³¼ ìƒì„¸ ì‹œê°í™”\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "eval_output_path = Path(\"evals/eval-output.json\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š AGENT EVALUATION RESULTS (ìƒì„¸)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "if not eval_output_path.exists():\n",
    "    print(\"âŒ í‰ê°€ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì…€ 5ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\\n\")\n",
    "else:\n",
    "    with open(eval_output_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metrics = data.get('metrics', {})\n",
    "    rows = data.get('rows', [])\n",
    "    \n",
    "    # 1. ì „ì²´ í‰ê·  ì ìˆ˜\n",
    "    print(\"â­ ì „ì²´ í‰ê·  ì„±ëŠ¥ ì ìˆ˜\")\n",
    "    print(\"-\" * 80)\n",
    "    scores = [\n",
    "        ('Intent Resolution', 'intent_resolution.intent_resolution', 'ì˜ë„ íŒŒì•…'),\n",
    "        ('Task Adherence', 'task_adherence.task_adherence', 'ì‘ì—… ì¶©ì‹¤ë„'),\n",
    "    ]\n",
    "    \n",
    "    for name, key, desc in scores:\n",
    "        if key in metrics:\n",
    "            score = metrics[key]\n",
    "            stars = 'â˜…' * int(score) + 'â˜†' * (5 - int(score))\n",
    "            bar = 'â–ˆ' * int(score * 4) + 'â–‘' * (20 - int(score * 4))\n",
    "            print(f\"  {name:20} {score:.2f}/5.0  {stars}  [{bar}]\")\n",
    "            print(f\"  {'':20} â†’ {desc}\")\n",
    "    \n",
    "    # 2. ìš´ì˜ ë©”íŠ¸ë¦­\n",
    "    print(\"\\n\\nâš¡ ìš´ì˜ ë©”íŠ¸ë¦­ (í‰ê· )\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    operational_keys = [\n",
    "        ('operational_metrics.server-run-duration-in-seconds', 'ì„œë²„ ì‹¤í–‰ ì‹œê°„', 's'),\n",
    "        ('operational_metrics.client-run-duration-in-seconds', 'í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰ ì‹œê°„', 's'),\n",
    "        ('operational_metrics.prompt-tokens', 'í”„ë¡¬í”„íŠ¸ í† í°', 'tokens'),\n",
    "        ('operational_metrics.completion-tokens', 'ì™„ì„± í† í°', 'tokens'),\n",
    "    ]\n",
    "    \n",
    "    for key, desc, unit in operational_keys:\n",
    "        if key in metrics:\n",
    "            value = metrics[key]\n",
    "            if unit == 'tokens':\n",
    "                print(f\"  {desc:25} {int(value):>8,} {unit}\")\n",
    "            else:\n",
    "                print(f\"  {desc:25} {value:>8.2f} {unit}\")\n",
    "    \n",
    "    # 3. ê°œë³„ ì¿¼ë¦¬ ê²°ê³¼\n",
    "    if rows:\n",
    "        print(\"\\n\\nğŸ“‹ ì¿¼ë¦¬ë³„ ìƒì„¸ ê²°ê³¼\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            query = \"\"\n",
    "            query_input = row.get('inputs.query', [])\n",
    "            if isinstance(query_input, list):\n",
    "                for item in query_input:\n",
    "                    if isinstance(item, dict) and item.get('role') == 'user':\n",
    "                        content = item.get('content', [])\n",
    "                        if isinstance(content, list) and len(content) > 0:\n",
    "                            query = content[0].get('text', '')\n",
    "                        break\n",
    "            \n",
    "            print(f\"\\n[Query {idx}]\")\n",
    "            print(f\"ì§ˆë¬¸: {query[:100]}{'...' if len(query) > 100 else ''}\")\n",
    "            \n",
    "            # ì ìˆ˜\n",
    "            intent = row.get('outputs.intent_resolution.intent_resolution', 'N/A')\n",
    "            task = row.get('outputs.task_adherence.task_adherence', 'N/A')\n",
    "            tool = row.get('outputs.tool_call_accuracy.tool_call_accuracy', 'N/A')\n",
    "            \n",
    "            print(f\"  Intent Resolution:  {intent if isinstance(intent, str) else f'{intent:.1f}/5.0'}\")\n",
    "            print(f\"  Task Adherence:     {task if isinstance(task, str) else f'{task:.1f}/5.0'}\")\n",
    "            print(f\"  Tool Call Accuracy: {tool}\")\n",
    "            \n",
    "            # ìš´ì˜ ë©”íŠ¸ë¦­\n",
    "            duration = row.get('outputs.operational_metrics.client-run-duration-in-seconds', 0)\n",
    "            prompt_tokens = row.get('outputs.operational_metrics.prompt-tokens', 0)\n",
    "            completion_tokens = row.get('outputs.operational_metrics.completion-tokens', 0)\n",
    "            \n",
    "            print(f\"  ì‹¤í–‰ ì‹œê°„: {duration:.2f}s  |  í† í°: {prompt_tokens} + {completion_tokens} = {prompt_tokens + completion_tokens}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # 4. í†µê³„ ìš”ì•½\n",
    "        print(\"\\n\\nğŸ“ˆ í†µê³„ ìš”ì•½\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        intent_scores = []\n",
    "        task_scores = []\n",
    "        durations = []\n",
    "        total_tokens = []\n",
    "        \n",
    "        for row in rows:\n",
    "            intent = row.get('outputs.intent_resolution.intent_resolution')\n",
    "            task = row.get('outputs.task_adherence.task_adherence')\n",
    "            duration = row.get('outputs.operational_metrics.client-run-duration-in-seconds', 0)\n",
    "            prompt = row.get('outputs.operational_metrics.prompt-tokens', 0)\n",
    "            completion = row.get('outputs.operational_metrics.completion-tokens', 0)\n",
    "            \n",
    "            if isinstance(intent, (int, float)):\n",
    "                intent_scores.append(intent)\n",
    "            if isinstance(task, (int, float)):\n",
    "                task_scores.append(task)\n",
    "            if duration:\n",
    "                durations.append(duration)\n",
    "            total_tokens.append(prompt + completion)\n",
    "        \n",
    "        if intent_scores:\n",
    "            print(f\"Intent Resolution í‰ê· :  {sum(intent_scores)/len(intent_scores):.2f}/5.0\")\n",
    "            print(f\"  ìµœê³ : {max(intent_scores):.1f}  |  ìµœì €: {min(intent_scores):.1f}\")\n",
    "        \n",
    "        if task_scores:\n",
    "            print(f\"\\nTask Adherence í‰ê· :     {sum(task_scores)/len(task_scores):.2f}/5.0\")\n",
    "            print(f\"  ìµœê³ : {max(task_scores):.1f}  |  ìµœì €: {min(task_scores):.1f}\")\n",
    "        \n",
    "        if durations:\n",
    "            print(f\"\\nì‹¤í–‰ ì‹œê°„ í‰ê· :           {sum(durations)/len(durations):.2f}s\")\n",
    "            print(f\"  ìµœê³ : {max(durations):.2f}s  |  ìµœì €: {min(durations):.2f}s\")\n",
    "        \n",
    "        if total_tokens:\n",
    "            print(f\"\\ní† í° ì‚¬ìš© í‰ê· :           {sum(total_tokens)/len(total_tokens):.0f} tokens\")\n",
    "            print(f\"  ìµœëŒ€: {max(total_tokens)}  |  ìµœì†Œ: {min(total_tokens)}\")\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(f\"âœ… ì´ {len(rows)}ê°œ ì¿¼ë¦¬ í‰ê°€ ì™„ë£Œ\")\n",
    "    print(f\"ğŸ“ ìƒì„¸ JSON: {eval_output_path}\")\n",
    "    print(f\"ğŸ’¡ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰: python3 show_eval_results.py\")\n",
    "    print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4900276",
   "metadata": {},
   "source": [
    "## 7. Evaluation ë©”íŠ¸ë¦­ í•´ì„ ê°€ì´ë“œ\n",
    "\n",
    "### ë©”íŠ¸ë¦­ í•´ì„\n",
    "\n",
    "**Operational Metrics:**\n",
    "- `server-run-duration-in-seconds`: ì„œë²„ì—ì„œ Agent ì‹¤í–‰ ì‹œê°„\n",
    "- `client-run-duration-in-seconds`: í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì¸¡ì •í•œ ì´ ì†Œìš” ì‹œê°„\n",
    "- `prompt-tokens`: ì…ë ¥ í† í° ìˆ˜\n",
    "- `completion-tokens`: ìƒì„±ëœ í† í° ìˆ˜\n",
    "\n",
    "**Performance Metrics:**\n",
    "- `tool_call_accuracy.*`: ë„êµ¬ í˜¸ì¶œ ì •í™•ë„ (1-5ì )\n",
    "- `intent_resolution.*`: ì˜ë„ íŒŒì•… ì •í™•ë„ (1-5ì )\n",
    "- `task_adherence.*`: ì‘ì—… ì¤€ìˆ˜ë„ (1-5ì )\n",
    "\n",
    "**ì ìˆ˜ í•´ì„:**\n",
    "- 5ì : ì™„ë²½í•¨\n",
    "- 4ì : ì¢‹ìŒ\n",
    "- 3ì : ë³´í†µ\n",
    "- 2ì : ê°œì„  í•„ìš”\n",
    "- 1ì : ë§¤ìš° ë‚˜ì¨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1fd41",
   "metadata": {},
   "source": [
    "## 8. ë‹¤ìŒ ë‹¨ê³„ ë° Agent ê°œì„ \n",
    "\n",
    "### Agent ê°œì„  ë°©ë²•\n",
    "\n",
    "1. **ë‚®ì€ ì ìˆ˜ ë¶„ì„**\n",
    "   - ì–´ë–¤ ì¿¼ë¦¬ì—ì„œ ì ìˆ˜ê°€ ë‚®ì•˜ëŠ”ì§€ í™•ì¸\n",
    "   - ì–´ë–¤ evaluatorì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆëŠ”ì§€ íŒŒì•…\n",
    "\n",
    "2. **í”„ë¡¬í”„íŠ¸ ê°œì„ **\n",
    "   - Agent instructionsë¥¼ ë” ëª…í™•í•˜ê²Œ ì‘ì„±\n",
    "   - ì˜ˆì‹œ ì¶”ê°€\n",
    "   - ì œì•½ì‚¬í•­ ëª…ì‹œ\n",
    "\n",
    "3. **ê¸°ëŠ¥ ê°œì„ **\n",
    "   - Tool ì¶”ê°€ ë˜ëŠ” ê°œì„ \n",
    "   - RAG ì§€ì‹ ë² ì´ìŠ¤ ë³´ê°•\n",
    "   - Multi-Agent ì¡°ì •\n",
    "\n",
    "4. **ì¬í‰ê°€**\n",
    "   - ë™ì¼í•œ ì¿¼ë¦¬ë¡œ ì¬í‰ê°€\n",
    "   - ì ìˆ˜ ë³€í™” ë¹„êµ\n",
    "   - ì§€ì†ì ì¸ ê°œì„ \n",
    "\n",
    "### ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [Azure AI Foundry Agent Evaluation](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)\n",
    "- [Built-in Evaluators](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk)\n",
    "- [Evaluation Best Practices](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-approach-gen-ai)\n",
    "\n",
    "---\n",
    "\n",
    "## ì™„ë£Œ!\n",
    "\n",
    "ì¶•í•˜í•©ë‹ˆë‹¤! Agent Evaluationì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

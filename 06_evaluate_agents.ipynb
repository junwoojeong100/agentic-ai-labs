{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eea5182",
   "metadata": {},
   "source": [
    "# 06. Agent Evaluation (Agent 평가)\n",
    "\n",
    "이 노트북에서는 배포된 Agent의 품질과 성능을 평가합니다.\n",
    "\n",
    "## 목표\n",
    "\n",
    "- Azure AI Evaluation SDK를 사용한 Agent 평가\n",
    "- 성능, 정확도, 안전성 메트릭 수집\n",
    "- 평가 결과 분석 및 시각화\n",
    "\n",
    "## 사전 요구사항\n",
    "\n",
    "1. Notebook 01-03 완료 (Azure 리소스 및 Agent 배포)\n",
    "2. `config.json` 파일 존재\n",
    "3. Azure AI Project 접근 권한\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784224b9",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 Config 로드\n",
    "\n",
    "이전 노트북에서 생성한 설정을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eebcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# config.json 로드\n",
    "config_path = Path(\"config.json\")\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(\"config.json not found. Please run notebooks 01-03 first.\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# 주요 변수 추출\n",
    "PROJECT_CONNECTION_STRING = config.get(\"project_connection_string\", \"\")\n",
    "simple_project_conn = PROJECT_CONNECTION_STRING.split(';')[0] if PROJECT_CONNECTION_STRING else \"\"\n",
    "\n",
    "print(\"=== Configuration Loaded ===\")\n",
    "print(f\"Resource Group: {config.get('resource_group')}\")\n",
    "print(f\"Location: {config.get('location')}\")\n",
    "print(f\"Project: {simple_project_conn}\")\n",
    "print(f\"Model: {config.get('model_deployment_name')}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc8337",
   "metadata": {},
   "source": [
    "## 2. Azure AI Evaluation 패키지 설치\n",
    "\n",
    "Agent 평가에 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Evaluation 패키지 설치\n",
    "print(\"=== Installing Azure AI Evaluation Package ===\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"azure-ai-evaluation\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ azure-ai-evaluation installed successfully\")\n",
    "else:\n",
    "    print(f\"⚠️  Installation warning: {result.stderr}\")\n",
    "    print(\"   Proceeding anyway...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b155d",
   "metadata": {},
   "source": [
    "## 3. Evaluation 개요\n",
    "\n",
    "### Evaluation 유형\n",
    "\n",
    "**Performance Evaluators (성능 평가):**\n",
    "- **Intent Resolution**: Agent가 사용자 의도를 올바르게 파악했는지 평가\n",
    "- **Tool Call Accuracy**: Agent가 올바른 도구를 정확하게 호출했는지 평가  \n",
    "- **Task Adherence**: Agent가 지시사항을 충실히 따랐는지 평가\n",
    "\n",
    "**Safety Evaluators (안전성 평가):**\n",
    "- **Content Safety**: 부적절한 콘텐츠(폭력, 혐오 등) 포함 여부 평가\n",
    "- **Indirect Attack**: 간접적인 악의적 공격 시도 감지\n",
    "- **Code Vulnerability**: 생성된 코드의 보안 취약점 평가\n",
    "\n",
    "### Evaluation 프로세스\n",
    "\n",
    "1. 테스트 쿼리 생성\n",
    "2. Agent 생성 및 쿼리 실행\n",
    "3. 응답 수집 및 메트릭 측정\n",
    "4. Evaluator로 품질 평가\n",
    "5. 결과 저장 및 분석\n",
    "6. Agent 정리\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4deab",
   "metadata": {},
   "source": [
    "## 4. 테스트 쿼리 생성\n",
    "\n",
    "Agent의 다양한 기능을 테스트할 쿼리를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evals 디렉토리 생성\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "evals_dir = Path(\"evals\")\n",
    "evals_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=== Creating Evaluation Test Queries ===\\n\")\n",
    "\n",
    "# 테스트 쿼리 정의\n",
    "eval_queries = [\n",
    "    {\n",
    "        \"query\": \"안녕하세요. 가족 여행지를 추천받고 싶어요.\",\n",
    "        \"ground-truth\": \"Agent는 인사에 응답하고, 여행지 추천을 위해 Research Agent를 호출해야 합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"부산의 현재 날씨를 알려주세요. 온도와 체감온도를 포함해주세요.\",\n",
    "        \"ground-truth\": \"부산의 현재 날씨 정보를 정확하게 제공해야 하며, 온도와 체감온도를 모두 포함해야 합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"제주도에서 가족과 함께 즐길 수 있는 여행지는 어디인가요?\",\n",
    "        \"ground-truth\": \"제주도의 가족 친화적인 여행지를 검색하여 추천해야 합니다. 자연 명소, 체험 활동, 접근성 등을 고려한 정보를 제공합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"서핑할 수 있는 해변을 추천해주세요.\",\n",
    "        \"ground-truth\": \"서핑이 가능한 한국의 해변 여행지를 검색하여 추천해야 합니다. 양양, 부산 등의 서핑 명소 정보를 제공합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"가을에 가기 좋은 단풍 명소는 어디인가요?\",\n",
    "        \"ground-truth\": \"가을 계절에 방문하기 좋은 단풍 명소를 검색하여 추천해야 합니다. 내장산, 설악산 등의 자연 명소 정보를 제공합니다.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# JSON 파일로 저장\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "with open(eval_queries_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Created {eval_queries_path}\")\n",
    "print(f\"\\n📋 Test Queries ({len(eval_queries)} total):\\n\")\n",
    "\n",
    "for i, query in enumerate(eval_queries, 1):\n",
    "    print(f\"   {i}. {query['query'][:60]}...\")\n",
    "\n",
    "print(\"\\n💡 각 쿼리는 Agent의 다른 기능을 테스트합니다:\")\n",
    "print(\"   • 일반 대화 및 여행 의도 파악\")\n",
    "print(\"   • 날씨 조회 (Tool 기능)\")\n",
    "print(\"   • 여행지 지식 검색 (RAG 기능)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded78b6",
   "metadata": {},
   "source": [
    "## 5. Agent Evaluation 실행\n",
    "\n",
    "평가용 Agent를 생성하고 테스트 쿼리를 실행한 후, Evaluator로 품질을 평가합니다.\n",
    "\n",
    "### 참고사항\n",
    "\n",
    "- **실행 시간**: 약 2-3분 소요\n",
    "- **리전 제약**: eastus 리전에서는 일부 Safety Evaluator가 지원되지 않습니다\n",
    "- **권한 이슈**: Azure AI Project의 스토리지 권한이 필요할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Evaluation 실행\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from azure.ai.agents.models import RunStatus, MessageRole\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.evaluation import (\n",
    "    AIAgentConverter, evaluate, ToolCallAccuracyEvaluator, IntentResolutionEvaluator, \n",
    "    TaskAdherenceEvaluator\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"=== Running Agent Evaluation ===\\n\")\n",
    "\n",
    "# 파일 경로 설정\n",
    "current_dir = Path(\".\")\n",
    "evals_dir = current_dir / \"evals\"\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "eval_input_path = evals_dir / \"eval-input.jsonl\"\n",
    "eval_output_path = evals_dir / \"eval-output.json\"\n",
    "\n",
    "# 환경 변수 로드 (이미 config에서 로드됨)\n",
    "project_endpoint = simple_project_conn\n",
    "parsed_endpoint = urlparse(project_endpoint)\n",
    "model_endpoint = f\"{parsed_endpoint.scheme}://{parsed_endpoint.netloc}\"\n",
    "deployment_name = config.get(\"model_deployment_name\", \"gpt-4o\")\n",
    "\n",
    "print(f\"📋 Configuration:\")\n",
    "print(f\"   Project: {project_endpoint}\")\n",
    "print(f\"   Model: {deployment_name}\")\n",
    "print(f\"   Test Queries: {eval_queries_path}\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Initialize AIProjectClient\n",
    "print(\"🔌 Connecting to AI Project...\")\n",
    "credential = DefaultAzureCredential()\n",
    "ai_project = AIProjectClient(\n",
    "    credential=credential,\n",
    "    endpoint=project_endpoint,\n",
    "    api_version=\"2025-05-15-preview\"  # Evaluations require preview API\n",
    ")\n",
    "print(\"✅ Connected\\n\")\n",
    "\n",
    "# Evaluation용 Agent 생성\n",
    "print(\"🤖 Creating Evaluation Agent...\")\n",
    "eval_agent = ai_project.agents.create_agent(\n",
    "    model=deployment_name,\n",
    "    name=\"Evaluation Agent\",\n",
    "    instructions=\"\"\"You are a helpful travel and weather assistant.\n",
    "    \n",
    "You can help users with:\n",
    "1. Travel recommendations and destination information\n",
    "2. Weather information for any city\n",
    "3. General travel planning advice\n",
    "\n",
    "Be friendly, informative, and provide detailed responses.\"\"\"\n",
    ")\n",
    "print(f\"✅ Agent created: {eval_agent.name} (ID: {eval_agent.id})\\n\")\n",
    "\n",
    "# Setup evaluation config\n",
    "api_version = config.get(\"api_version\", \"2024-08-01-preview\")\n",
    "model_config = {\n",
    "    \"azure_deployment\": deployment_name,\n",
    "    \"azure_endpoint\": model_endpoint,\n",
    "    \"api_version\": api_version,\n",
    "}\n",
    "\n",
    "thread_data_converter = AIAgentConverter(ai_project)\n",
    "\n",
    "# 테스트 쿼리 실행 및 evaluation input 준비\n",
    "print(\"=\"*70)\n",
    "print(\"📝 Executing Test Queries\\n\")\n",
    "\n",
    "with open(eval_queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"   Total queries: {len(test_data)}\\n\")\n",
    "\n",
    "with open(eval_input_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, row in enumerate(test_data, 1):\n",
    "        query_text = row.get(\"query\")\n",
    "        print(f\"   [{idx}/{len(test_data)}] {query_text[:50]}...\")\n",
    "        \n",
    "        # 새 스레드 생성 (각 쿼리를 격리)\n",
    "        thread = ai_project.agents.threads.create()\n",
    "        \n",
    "        # 사용자 쿼리 생성\n",
    "        ai_project.agents.messages.create(\n",
    "            thread.id, role=MessageRole.USER, content=query_text\n",
    "        )\n",
    "        \n",
    "        # Agent 실행 및 성능 측정\n",
    "        start_time = time.time()\n",
    "        run = ai_project.agents.runs.create_and_process(\n",
    "            thread_id=thread.id, agent_id=eval_agent.id\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if run.status != RunStatus.COMPLETED:\n",
    "            print(f\"      ⚠️  Run failed: {run.last_error or 'Unknown error'}\")\n",
    "            continue\n",
    "        \n",
    "        # 운영 메트릭 수집\n",
    "        operational_metrics = {\n",
    "            \"server-run-duration-in-seconds\": (\n",
    "                run.completed_at - run.created_at\n",
    "            ).total_seconds(),\n",
    "            \"client-run-duration-in-seconds\": end_time - start_time,\n",
    "            \"completion-tokens\": run.usage.completion_tokens,\n",
    "            \"prompt-tokens\": run.usage.prompt_tokens,\n",
    "            \"ground-truth\": row.get(\"ground-truth\", '')\n",
    "        }\n",
    "        \n",
    "        # Thread 데이터 + 운영 메트릭을 evaluation input에 추가\n",
    "        evaluation_data = thread_data_converter.prepare_evaluation_data(thread_ids=thread.id)\n",
    "        eval_item = evaluation_data[0]\n",
    "        eval_item[\"metrics\"] = operational_metrics\n",
    "        f.write(json.dumps(eval_item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"      ✅ Completed in {operational_metrics['client-run-duration-in-seconds']:.1f}s\")\n",
    "        print(f\"         Tokens: {operational_metrics['prompt-tokens']} prompt + {operational_metrics['completion-tokens']} completion\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ All queries executed successfully\")\n",
    "print(f\"   Input saved to: {eval_input_path}\\n\")\n",
    "\n",
    "# Evaluation 실행\n",
    "print(\"=\"*70)\n",
    "print(\"🔬 Running Evaluators\\n\")\n",
    "\n",
    "print(\"   Evaluators:\")\n",
    "print(\"      • ToolCallAccuracyEvaluator\")\n",
    "print(\"      • IntentResolutionEvaluator\")\n",
    "print(\"      • TaskAdherenceEvaluator\")\n",
    "print(\"\\n   ⚠️  참고: eastus 리전에서는 일부 RAI 평가자가 지원되지 않습니다.\")\n",
    "print(\"      (CodeVulnerability, ContentSafety, IndirectAttack는 제외)\\n\")\n",
    "print(\"   ⏳ This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# OperationalMetricsEvaluator 정의\n",
    "class OperationalMetricsEvaluator:\n",
    "    \"\"\"Propagate operational metrics to the final evaluation results\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, *, metrics: dict, **kwargs):\n",
    "        return metrics\n",
    "\n",
    "# Evaluation 실행 (로컬)\n",
    "print(\"   💡 평가 결과를 로컬에 저장합니다.\\n\")\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"foundry-agent-evaluation\",\n",
    "    data=eval_input_path,\n",
    "    evaluators={\n",
    "        \"operational_metrics\": OperationalMetricsEvaluator(),\n",
    "        \"tool_call_accuracy\": ToolCallAccuracyEvaluator(model_config=model_config),\n",
    "        \"intent_resolution\": IntentResolutionEvaluator(model_config=model_config),\n",
    "        \"task_adherence\": TaskAdherenceEvaluator(model_config=model_config),\n",
    "        # eastus 리전에서 지원되지 않는 평가자들은 제외\n",
    "        # \"code_vulnerability\": CodeVulnerabilityEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"content_safety\": ContentSafetyEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"indirect_attack\": IndirectAttackEvaluator(credential=credential, azure_ai_project=project_endpoint)\n",
    "    },\n",
    "    output_path=eval_output_path,\n",
    "    # azure_ai_project 파라미터 제거 (ML workspace 불필요)\n",
    ")\n",
    "\n",
    "print(\"✅ Evaluation completed!\\n\")\n",
    "print(f\"📁 결과 저장 위치: {eval_output_path}\")\n",
    "print(f\"   다음 셀에서 결과를 확인하세요.\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluation Agent 삭제\n",
    "print(\"🧹 Cleaning up...\")\n",
    "ai_project.agents.delete_agent(eval_agent.id)\n",
    "print(f\"✅ Evaluation Agent deleted: {eval_agent.id}\\n\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dbb91",
   "metadata": {},
   "source": [
    "## 6. Evaluation 결과 시각화\n",
    "\n",
    "결과를 표와 차트로 시각화하여 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation 결과 시각화\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== 📊 Evaluation Results (평가 결과) ===\\n\")\n",
    "\n",
    "# 파일에서 결과 로드\n",
    "eval_output_path = Path(\"evals/eval-output.json\")\n",
    "\n",
    "if not eval_output_path.exists():\n",
    "    print(\"❌ 평가 결과 파일을 찾을 수 없습니다.\")\n",
    "    print(f\"   파일 경로: {eval_output_path}\")\n",
    "    print(\"\\n   먼저 Cell 5를 실행하여 평가를 수행해주세요.\\n\")\n",
    "else:\n",
    "    with open(eval_output_path, 'r', encoding='utf-8') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    metrics = results.get('metrics', {})\n",
    "    rows = results.get('rows', [])\n",
    "    \n",
    "    # 1. 전체 점수 요약\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"⭐ OVERALL PERFORMANCE SCORES (전체 성능 점수)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    performance_scores = {}\n",
    "    for key, value in metrics.items():\n",
    "        if any(x in key.lower() for x in ['accuracy', 'resolution', 'adherence']) and isinstance(value, (int, float)):\n",
    "            # 점수 이름 정리\n",
    "            score_name = key.replace('.', ' ').replace('_', ' ').title()\n",
    "            performance_scores[score_name] = value\n",
    "    \n",
    "    if performance_scores:\n",
    "        for score_name, score in sorted(performance_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "            # 1-5 점수를 별표와 프로그레스 바로 표시\n",
    "            stars = '★' * int(score) + '☆' * (5 - int(score))\n",
    "            bar_length = int(score * 10)\n",
    "            bar = '█' * bar_length + '░' * (50 - bar_length)\n",
    "            print(f\"{score_name:.<40} {score:.2f}/5.0  {stars}\")\n",
    "            print(f\"{'':.<40} {bar}\\n\")\n",
    "    else:\n",
    "        print(\"   (성능 점수 데이터 없음)\\n\")\n",
    "    \n",
    "    # 2. 운영 메트릭 요약\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"⚡ OPERATIONAL METRICS (운영 메트릭)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    operational_data = {}\n",
    "    for key, value in metrics.items():\n",
    "        if 'duration' in key.lower():\n",
    "            operational_data[f\"⏱️  {key.replace('_', ' ').replace('-', ' ').title()}\"] = f\"{value:.2f}s\"\n",
    "        elif 'token' in key.lower():\n",
    "            operational_data[f\"🔤 {key.replace('_', ' ').replace('-', ' ').title()}\"] = f\"{int(value):,}\"\n",
    "    \n",
    "    if operational_data:\n",
    "        for metric_name, value in operational_data.items():\n",
    "            print(f\"   {metric_name:.<65} {value:>10}\")\n",
    "    else:\n",
    "        print(\"   (운영 메트릭 데이터 없음)\")\n",
    "    \n",
    "    # 3. 개별 쿼리 결과 상세 테이블\n",
    "    print(\"\\n\\n\" + \"=\"*120)\n",
    "    print(\"📋 DETAILED QUERY RESULTS (쿼리별 상세 결과)\")\n",
    "    print(\"=\"*120 + \"\\n\")\n",
    "    \n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # 주요 컬럼 선택 및 정리\n",
    "        display_data = []\n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            query_text = row.get('query', '')[:45] + '...' if len(row.get('query', '')) > 45 else row.get('query', '')\n",
    "            \n",
    "            result_row = {\n",
    "                'No.': idx,\n",
    "                'Query': query_text,\n",
    "                'Intent': row.get('intent_resolution.intent_resolution', '-'),\n",
    "                'Task': row.get('task_adherence.task_adherence', '-'),\n",
    "                'Tool': row.get('tool_call_accuracy.tool_call_accuracy', '-'),\n",
    "                'Duration(s)': f\"{row.get('client-run-duration-in-seconds', 0):.1f}\",\n",
    "                'Tokens': f\"{row.get('prompt-tokens', 0)}+{row.get('completion-tokens', 0)}\"\n",
    "            }\n",
    "            display_data.append(result_row)\n",
    "        \n",
    "        result_df = pd.DataFrame(display_data)\n",
    "        print(result_df.to_string(index=False))\n",
    "        \n",
    "        # 평균 점수 계산\n",
    "        print(\"\\n\" + \"-\"*120)\n",
    "        intent_scores = [r.get('intent_resolution.intent_resolution') for r in rows if isinstance(r.get('intent_resolution.intent_resolution'), (int, float))]\n",
    "        task_scores = [r.get('task_adherence.task_adherence') for r in rows if isinstance(r.get('task_adherence.task_adherence'), (int, float))]\n",
    "        tool_scores = [r.get('tool_call_accuracy.tool_call_accuracy') for r in rows if isinstance(r.get('tool_call_accuracy.tool_call_accuracy'), (int, float))]\n",
    "        \n",
    "        if intent_scores:\n",
    "            print(f\"\\n📈 Average Scores:\")\n",
    "            if intent_scores: print(f\"   Intent Resolution:  {sum(intent_scores)/len(intent_scores):.2f}/5.0\")\n",
    "            if task_scores: print(f\"   Task Adherence:     {sum(task_scores)/len(task_scores):.2f}/5.0\")\n",
    "            if tool_scores: print(f\"   Tool Call Accuracy: {sum(tool_scores)/len(tool_scores):.2f}/5.0\")\n",
    "    else:\n",
    "        print(\"   (쿼리 결과 없음)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    \n",
    "    # 4. 점수 분포 차트\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"📊 SCORE DISTRIBUTION (점수 분포)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    if rows:\n",
    "        # 각 메트릭별 점수 분포 계산\n",
    "        for metric_name, scores in [\n",
    "            ('Intent Resolution', intent_scores),\n",
    "            ('Task Adherence', task_scores),\n",
    "            ('Tool Call Accuracy', tool_scores)\n",
    "        ]:\n",
    "            if scores:\n",
    "                avg_score = sum(scores) / len(scores)\n",
    "                print(f\"\\n{metric_name}:\")\n",
    "                \n",
    "                # 히스토그램\n",
    "                for score in range(5, 0, -1):\n",
    "                    count = sum(1 for s in scores if int(s) == score)\n",
    "                    bar = '█' * count + '░' * (len(scores) - count)\n",
    "                    print(f\"   {score} {'★' * score:5}  [{count:2}]  {bar}\")\n",
    "                \n",
    "                print(f\"   {'─' * 40}\")\n",
    "                print(f\"   Average: {avg_score:.2f}/5.0\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # 5. 파일 저장 위치\n",
    "    print(f\"\\n\\n✅ 평가 완료!\")\n",
    "    print(f\"\\n📁 상세 결과 파일:\")\n",
    "    print(f\"   • {eval_output_path}\")\n",
    "    print(f\"   • evals/eval-input.jsonl\")\n",
    "    print(f\"\\n💡 Tip: 위 파일을 열어서 더 자세한 내용을 확인할 수 있습니다.\\n\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4900276",
   "metadata": {},
   "source": [
    "## 7. Evaluation 메트릭 해석 가이드\n",
    "\n",
    "### 메트릭 해석\n",
    "\n",
    "**Operational Metrics:**\n",
    "- `server-run-duration-in-seconds`: 서버에서 Agent 실행 시간\n",
    "- `client-run-duration-in-seconds`: 클라이언트에서 측정한 총 소요 시간\n",
    "- `prompt-tokens`: 입력 토큰 수\n",
    "- `completion-tokens`: 생성된 토큰 수\n",
    "\n",
    "**Performance Metrics:**\n",
    "- `tool_call_accuracy.*`: 도구 호출 정확도 (1-5점)\n",
    "- `intent_resolution.*`: 의도 파악 정확도 (1-5점)\n",
    "- `task_adherence.*`: 작업 준수도 (1-5점)\n",
    "\n",
    "**점수 해석:**\n",
    "- 5점: 완벽함\n",
    "- 4점: 좋음\n",
    "- 3점: 보통\n",
    "- 2점: 개선 필요\n",
    "- 1점: 매우 나쁨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1fd41",
   "metadata": {},
   "source": [
    "## 8. 다음 단계 및 Agent 개선\n",
    "\n",
    "### Agent 개선 방법\n",
    "\n",
    "1. **낮은 점수 분석**\n",
    "   - 어떤 쿼리에서 점수가 낮았는지 확인\n",
    "   - 어떤 evaluator에서 문제가 발생했는지 파악\n",
    "\n",
    "2. **프롬프트 개선**\n",
    "   - Agent instructions를 더 명확하게 작성\n",
    "   - 예시 추가\n",
    "   - 제약사항 명시\n",
    "\n",
    "3. **기능 개선**\n",
    "   - Tool 추가 또는 개선\n",
    "   - RAG 지식 베이스 보강\n",
    "   - Multi-Agent 조정\n",
    "\n",
    "4. **재평가**\n",
    "   - 동일한 쿼리로 재평가\n",
    "   - 점수 변화 비교\n",
    "   - 지속적인 개선\n",
    "\n",
    "### 참고 자료\n",
    "\n",
    "- [Azure AI Foundry Agent Evaluation](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)\n",
    "- [Built-in Evaluators](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk)\n",
    "- [Evaluation Best Practices](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-approach-gen-ai)\n",
    "\n",
    "---\n",
    "\n",
    "## 완료!\n",
    "\n",
    "축하합니다! Agent Evaluation을 성공적으로 완료했습니다. 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eea5182",
   "metadata": {},
   "source": [
    "# 06. Agent Evaluation (Agent í‰ê°€)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë°°í¬ëœ Agentì˜ í’ˆì§ˆê³¼ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "\n",
    "- Azure AI Evaluation SDKë¥¼ ì‚¬ìš©í•œ Agent í‰ê°€\n",
    "- ì„±ëŠ¥, ì •í™•ë„, ì•ˆì „ì„± ë©”íŠ¸ë¦­ ìˆ˜ì§‘\n",
    "- í‰ê°€ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
    "\n",
    "## ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
    "\n",
    "1. Notebook 01-03 ì™„ë£Œ (Azure ë¦¬ì†ŒìŠ¤ ë° Agent ë°°í¬)\n",
    "2. `config.json` íŒŒì¼ ì¡´ì¬\n",
    "3. Azure AI Project ì ‘ê·¼ ê¶Œí•œ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b3082",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš™ï¸ ì‹œì‘í•˜ê¸° ì „ì— (Before You Start)\n",
    "\n",
    "**Python ì»¤ë„ì„ ì„ íƒí•˜ì„¸ìš”:**\n",
    "\n",
    "1. ë…¸íŠ¸ë¶ ì˜¤ë¥¸ìª½ ìƒë‹¨ì˜ **\"ì»¤ë„ ì„ íƒ (Select Kernel)\"** í´ë¦­\n",
    "2. **\"Python Environments...\"** ì„ íƒ\n",
    "3. **`.venv (Python 3.x.x)`** ì„ íƒ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìƒì„±ëœ ê°€ìƒí™˜ê²½)\n",
    "\n",
    "> ğŸ’¡ **GitHub Codespaces**: Codespacesì—ì„œëŠ” ìë™ìœ¼ë¡œ `.venv` í™˜ê²½ì´ ìƒì„±ë©ë‹ˆë‹¤.  \n",
    "> ë§Œì•½ `.venv`ê°€ ë³´ì´ì§€ ì•Šìœ¼ë©´ í„°ë¯¸ë„ì—ì„œ `python -m venv .venv`ë¡œ ìƒì„±í•˜ì„¸ìš”.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784224b9",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° Config ë¡œë“œ\n",
    "\n",
    "ì´ì „ ë…¸íŠ¸ë¶ì—ì„œ ìƒì„±í•œ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eebcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# config.json ë¡œë“œ\n",
    "config_path = Path(\"config.json\")\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(\"config.json not found. Please run notebooks 01-03 first.\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# ì£¼ìš” ë³€ìˆ˜ ì¶”ì¶œ\n",
    "PROJECT_CONNECTION_STRING = config.get(\"project_connection_string\", \"\")\n",
    "simple_project_conn = PROJECT_CONNECTION_STRING.split(';')[0] if PROJECT_CONNECTION_STRING else \"\"\n",
    "\n",
    "print(\"=== Configuration Loaded ===\")\n",
    "print(f\"Resource Group: {config.get('resource_group')}\")\n",
    "print(f\"Location: {config.get('location')}\")\n",
    "print(f\"Project: {simple_project_conn}\")\n",
    "print(f\"Model: {config.get('model_deployment_name')}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc8337",
   "metadata": {},
   "source": [
    "## 2. Azure AI Evaluation íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "Agent í‰ê°€ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Evaluation íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "print(\"=== Installing Azure AI Evaluation Package ===\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"azure-ai-evaluation\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"âœ… azure-ai-evaluation installed successfully\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Installation warning: {result.stderr}\")\n",
    "    print(\"   Proceeding anyway...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b155d",
   "metadata": {},
   "source": [
    "## 3. Evaluation ê°œìš”\n",
    "\n",
    "### Evaluation ìœ í˜•\n",
    "\n",
    "**Performance Evaluators (ì„±ëŠ¥ í‰ê°€):**\n",
    "- **Intent Resolution**: Agentê°€ ì‚¬ìš©ì ì˜ë„ë¥¼ ì˜¬ë°”ë¥´ê²Œ íŒŒì•…í–ˆëŠ”ì§€ í‰ê°€\n",
    "- **Tool Call Accuracy**: Agentê°€ ì˜¬ë°”ë¥¸ ë„êµ¬ë¥¼ ì •í™•í•˜ê²Œ í˜¸ì¶œí–ˆëŠ”ì§€ í‰ê°€  \n",
    "- **Task Adherence**: Agentê°€ ì§€ì‹œì‚¬í•­ì„ ì¶©ì‹¤íˆ ë”°ëëŠ”ì§€ í‰ê°€\n",
    "\n",
    "**Safety Evaluators (ì•ˆì „ì„± í‰ê°€):**\n",
    "- **Content Safety**: ë¶€ì ì ˆí•œ ì½˜í…ì¸ (í­ë ¥, í˜ì˜¤ ë“±) í¬í•¨ ì—¬ë¶€ í‰ê°€\n",
    "- **Indirect Attack**: ê°„ì ‘ì ì¸ ì•…ì˜ì  ê³µê²© ì‹œë„ ê°ì§€\n",
    "- **Code Vulnerability**: ìƒì„±ëœ ì½”ë“œì˜ ë³´ì•ˆ ì·¨ì•½ì  í‰ê°€\n",
    "\n",
    "### Evaluation í”„ë¡œì„¸ìŠ¤\n",
    "\n",
    "1. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±\n",
    "2. Agent ìƒì„± ë° ì¿¼ë¦¬ ì‹¤í–‰\n",
    "3. ì‘ë‹µ ìˆ˜ì§‘ ë° ë©”íŠ¸ë¦­ ì¸¡ì •\n",
    "4. Evaluatorë¡œ í’ˆì§ˆ í‰ê°€\n",
    "5. ê²°ê³¼ ì €ì¥ ë° ë¶„ì„\n",
    "6. Agent ì •ë¦¬\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4deab",
   "metadata": {},
   "source": [
    "## 4. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±\n",
    "\n",
    "Agentì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evals ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "evals_dir = Path(\"evals\")\n",
    "evals_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=== Creating Evaluation Test Queries ===\\n\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜\n",
    "eval_queries = [\n",
    "    {\n",
    "        \"query\": \"ì•ˆë…•í•˜ì„¸ìš”. ê°€ì¡± ì—¬í–‰ì§€ë¥¼ ì¶”ì²œë°›ê³  ì‹¶ì–´ìš”.\",\n",
    "        \"ground-truth\": \"AgentëŠ” ì¸ì‚¬ì— ì‘ë‹µí•˜ê³ , ì—¬í–‰ì§€ ì¶”ì²œì„ ìœ„í•´ Research Agentë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ë¶€ì‚°ì˜ í˜„ì¬ ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ì˜¨ë„ì™€ ì²´ê°ì˜¨ë„ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.\",\n",
    "        \"ground-truth\": \"ë¶€ì‚°ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì œê³µí•´ì•¼ í•˜ë©°, ì˜¨ë„ì™€ ì²´ê°ì˜¨ë„ë¥¼ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ì œì£¼ë„ì—ì„œ ê°€ì¡±ê³¼ í•¨ê»˜ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì—¬í–‰ì§€ëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
    "        \"ground-truth\": \"ì œì£¼ë„ì˜ ê°€ì¡± ì¹œí™”ì ì¸ ì—¬í–‰ì§€ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤. ìì—° ëª…ì†Œ, ì²´í—˜ í™œë™, ì ‘ê·¼ì„± ë“±ì„ ê³ ë ¤í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ì„œí•‘í•  ìˆ˜ ìˆëŠ” í•´ë³€ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.\",\n",
    "        \"ground-truth\": \"ì„œí•‘ì´ ê°€ëŠ¥í•œ í•œêµ­ì˜ í•´ë³€ ì—¬í–‰ì§€ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤. ì–‘ì–‘, ë¶€ì‚° ë“±ì˜ ì„œí•‘ ëª…ì†Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ê°€ì„ì— ê°€ê¸° ì¢‹ì€ ë‹¨í’ ëª…ì†ŒëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
    "        \"ground-truth\": \"ê°€ì„ ê³„ì ˆì— ë°©ë¬¸í•˜ê¸° ì¢‹ì€ ë‹¨í’ ëª…ì†Œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤. ë‚´ì¥ì‚°, ì„¤ì•…ì‚° ë“±ì˜ ìì—° ëª…ì†Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "with open(eval_queries_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Created {eval_queries_path}\")\n",
    "print(f\"\\nğŸ“‹ Test Queries ({len(eval_queries)} total):\\n\")\n",
    "\n",
    "for i, query in enumerate(eval_queries, 1):\n",
    "    print(f\"   {i}. {query['query'][:60]}...\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ê° ì¿¼ë¦¬ëŠ” Agentì˜ ë‹¤ë¥¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:\")\n",
    "print(\"   â€¢ ì¼ë°˜ ëŒ€í™” ë° ì—¬í–‰ ì˜ë„ íŒŒì•…\")\n",
    "print(\"   â€¢ ë‚ ì”¨ ì¡°íšŒ (Tool ê¸°ëŠ¥)\")\n",
    "print(\"   â€¢ ì—¬í–‰ì§€ ì§€ì‹ ê²€ìƒ‰ (RAG ê¸°ëŠ¥)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded78b6",
   "metadata": {},
   "source": [
    "## 5. Agent Evaluation ì‹¤í–‰\n",
    "\n",
    "í‰ê°€ìš© Agentë¥¼ ìƒì„±í•˜ê³  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•œ í›„, Evaluatorë¡œ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì°¸ê³ ì‚¬í•­\n",
    "\n",
    "- **ì‹¤í–‰ ì‹œê°„**: ì•½ 2-3ë¶„ ì†Œìš”\n",
    "- **ë¦¬ì „ ì œì•½**: eastus ë¦¬ì „ì—ì„œëŠ” ì¼ë¶€ Safety Evaluatorê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\n",
    "- **ê¶Œí•œ ì´ìŠˆ**: Azure AI Projectì˜ ìŠ¤í† ë¦¬ì§€ ê¶Œí•œì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Evaluation ì‹¤í–‰\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from azure.ai.agents.models import RunStatus, MessageRole\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.evaluation import (\n",
    "    AIAgentConverter, evaluate, ToolCallAccuracyEvaluator, IntentResolutionEvaluator, \n",
    "    TaskAdherenceEvaluator\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"=== Running Agent Evaluation ===\\n\")\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "current_dir = Path(\".\")\n",
    "evals_dir = current_dir / \"evals\"\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "eval_input_path = evals_dir / \"eval-input.jsonl\"\n",
    "eval_output_path = evals_dir / \"eval-output.json\"\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ì´ë¯¸ configì—ì„œ ë¡œë“œë¨)\n",
    "project_endpoint = simple_project_conn\n",
    "parsed_endpoint = urlparse(project_endpoint)\n",
    "model_endpoint = f\"{parsed_endpoint.scheme}://{parsed_endpoint.netloc}\"\n",
    "deployment_name = config.get(\"model_deployment_name\", \"gpt-4o\")\n",
    "\n",
    "print(f\"ğŸ“‹ Configuration:\")\n",
    "print(f\"   Project: {project_endpoint}\")\n",
    "print(f\"   Model: {deployment_name}\")\n",
    "print(f\"   Test Queries: {eval_queries_path}\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Initialize AIProjectClient\n",
    "print(\"ğŸ”Œ Connecting to AI Project...\")\n",
    "credential = DefaultAzureCredential()\n",
    "ai_project = AIProjectClient(\n",
    "    credential=credential,\n",
    "    endpoint=project_endpoint,\n",
    "    api_version=\"2025-05-15-preview\"  # Evaluations require preview API\n",
    ")\n",
    "print(\"âœ… Connected\\n\")\n",
    "\n",
    "# Evaluationìš© Agent ìƒì„±\n",
    "print(\"ğŸ¤– Creating Evaluation Agent...\")\n",
    "eval_agent = ai_project.agents.create_agent(\n",
    "    model=deployment_name,\n",
    "    name=\"Evaluation Agent\",\n",
    "    instructions=\"\"\"You are a helpful travel and weather assistant.\n",
    "    \n",
    "You can help users with:\n",
    "1. Travel recommendations and destination information\n",
    "2. Weather information for any city\n",
    "3. General travel planning advice\n",
    "\n",
    "Be friendly, informative, and provide detailed responses.\"\"\"\n",
    ")\n",
    "print(f\"âœ… Agent created: {eval_agent.name} (ID: {eval_agent.id})\\n\")\n",
    "\n",
    "# Setup evaluation config\n",
    "api_version = config.get(\"api_version\", \"2024-08-01-preview\")\n",
    "model_config = {\n",
    "    \"azure_deployment\": deployment_name,\n",
    "    \"azure_endpoint\": model_endpoint,\n",
    "    \"api_version\": api_version,\n",
    "}\n",
    "\n",
    "thread_data_converter = AIAgentConverter(ai_project)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰ ë° evaluation input ì¤€ë¹„\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“ Executing Test Queries\\n\")\n",
    "\n",
    "with open(eval_queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"   Total queries: {len(test_data)}\\n\")\n",
    "\n",
    "with open(eval_input_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, row in enumerate(test_data, 1):\n",
    "        query_text = row.get(\"query\")\n",
    "        print(f\"   [{idx}/{len(test_data)}] {query_text[:50]}...\")\n",
    "        \n",
    "        # ìƒˆ ìŠ¤ë ˆë“œ ìƒì„± (ê° ì¿¼ë¦¬ë¥¼ ê²©ë¦¬)\n",
    "        thread = ai_project.agents.threads.create()\n",
    "        \n",
    "        # ì‚¬ìš©ì ì¿¼ë¦¬ ìƒì„±\n",
    "        ai_project.agents.messages.create(\n",
    "            thread.id, role=MessageRole.USER, content=query_text\n",
    "        )\n",
    "        \n",
    "        # Agent ì‹¤í–‰ ë° ì„±ëŠ¥ ì¸¡ì •\n",
    "        start_time = time.time()\n",
    "        run = ai_project.agents.runs.create_and_process(\n",
    "            thread_id=thread.id, agent_id=eval_agent.id\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if run.status != RunStatus.COMPLETED:\n",
    "            print(f\"      âš ï¸  Run failed: {run.last_error or 'Unknown error'}\")\n",
    "            continue\n",
    "        \n",
    "        # ìš´ì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘\n",
    "        operational_metrics = {\n",
    "            \"server-run-duration-in-seconds\": (\n",
    "                run.completed_at - run.created_at\n",
    "            ).total_seconds(),\n",
    "            \"client-run-duration-in-seconds\": end_time - start_time,\n",
    "            \"completion-tokens\": run.usage.completion_tokens,\n",
    "            \"prompt-tokens\": run.usage.prompt_tokens,\n",
    "            \"ground-truth\": row.get(\"ground-truth\", '')\n",
    "        }\n",
    "        \n",
    "        # Thread ë°ì´í„° + ìš´ì˜ ë©”íŠ¸ë¦­ì„ evaluation inputì— ì¶”ê°€\n",
    "        evaluation_data = thread_data_converter.prepare_evaluation_data(thread_ids=thread.id)\n",
    "        eval_item = evaluation_data[0]\n",
    "        eval_item[\"metrics\"] = operational_metrics\n",
    "        f.write(json.dumps(eval_item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"      âœ… Completed in {operational_metrics['client-run-duration-in-seconds']:.1f}s\")\n",
    "        print(f\"         Tokens: {operational_metrics['prompt-tokens']} prompt + {operational_metrics['completion-tokens']} completion\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ… All queries executed successfully\")\n",
    "print(f\"   Input saved to: {eval_input_path}\\n\")\n",
    "\n",
    "# Evaluation ì‹¤í–‰\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ”¬ Running Evaluators\\n\")\n",
    "\n",
    "print(\"   Evaluators:\")\n",
    "print(\"      â€¢ ToolCallAccuracyEvaluator\")\n",
    "print(\"      â€¢ IntentResolutionEvaluator\")\n",
    "print(\"      â€¢ TaskAdherenceEvaluator\")\n",
    "print(\"\\n   âš ï¸  ì°¸ê³ : eastus ë¦¬ì „ì—ì„œëŠ” ì¼ë¶€ RAI í‰ê°€ìê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "print(\"      (CodeVulnerability, ContentSafety, IndirectAttackëŠ” ì œì™¸)\\n\")\n",
    "print(\"   â³ This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# OperationalMetricsEvaluator ì •ì˜\n",
    "class OperationalMetricsEvaluator:\n",
    "    \"\"\"Propagate operational metrics to the final evaluation results\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, *, metrics: dict, **kwargs):\n",
    "        return metrics\n",
    "\n",
    "# Evaluation ì‹¤í–‰ (ë¡œì»¬)\n",
    "print(\"   ğŸ’¡ í‰ê°€ ê²°ê³¼ë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"foundry-agent-evaluation\",\n",
    "    data=eval_input_path,\n",
    "    evaluators={\n",
    "        \"operational_metrics\": OperationalMetricsEvaluator(),\n",
    "        \"tool_call_accuracy\": ToolCallAccuracyEvaluator(model_config=model_config),\n",
    "        \"intent_resolution\": IntentResolutionEvaluator(model_config=model_config),\n",
    "        \"task_adherence\": TaskAdherenceEvaluator(model_config=model_config),\n",
    "        # eastus ë¦¬ì „ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠëŠ” í‰ê°€ìë“¤ì€ ì œì™¸\n",
    "        # \"code_vulnerability\": CodeVulnerabilityEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"content_safety\": ContentSafetyEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"indirect_attack\": IndirectAttackEvaluator(credential=credential, azure_ai_project=project_endpoint)\n",
    "    },\n",
    "    output_path=eval_output_path,\n",
    "    # azure_ai_project íŒŒë¼ë¯¸í„° ì œê±° (ML workspace ë¶ˆí•„ìš”)\n",
    ")\n",
    "\n",
    "print(\"âœ… Evaluation completed!\\n\")\n",
    "print(f\"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {eval_output_path}\")\n",
    "print(f\"   ë‹¤ìŒ ì…€ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluation Agent ì‚­ì œ\n",
    "print(\"ğŸ§¹ Cleaning up...\")\n",
    "ai_project.agents.delete_agent(eval_agent.id)\n",
    "print(f\"âœ… Evaluation Agent deleted: {eval_agent.id}\\n\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dbb91",
   "metadata": {},
   "source": [
    "## 6. Evaluation ê²°ê³¼ ì‹œê°í™”\n",
    "\n",
    "ê²°ê³¼ë¥¼ í‘œì™€ ì°¨íŠ¸ë¡œ ì‹œê°í™”í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ê²°ê³¼ í™•ì¸ ë°©ë²•\n",
    "\n",
    "**ë°©ë²• 1: ë…¸íŠ¸ë¶ì—ì„œ í™•ì¸**\n",
    "- ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ë…¸íŠ¸ë¶ì—ì„œ ë°”ë¡œ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë°©ë²• 2: í„°ë¯¸ë„ì—ì„œ í™•ì¸**\n",
    "- í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ë™ì¼í•œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "  ```bash\n",
    "  python3 show_eval_results.py\n",
    "  ```\n",
    "- ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìë™ìœ¼ë¡œ ìƒì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation ê²°ê³¼ ìƒì„¸ ì‹œê°í™”\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "SEPARATOR = \"â”€\" * 100\n",
    "LINE = \"=\" * 100\n",
    "\n",
    "def get_score_color(score, threshold=3.0):\n",
    "    if score >= 4.5:\n",
    "        return \"\\033[92m\"\n",
    "    elif score >= threshold:\n",
    "        return \"\\033[93m\"\n",
    "    else:\n",
    "        return \"\\033[91m\"\n",
    "\n",
    "def reset_color():\n",
    "    return \"\\033[0m\"\n",
    "\n",
    "def get_score_indicator(score, threshold=3.0):\n",
    "    if score >= 4.5:\n",
    "        return \"âœ…\"\n",
    "    elif score >= threshold:\n",
    "        return \"âš ï¸\"\n",
    "    else:\n",
    "        return \"âŒ\"\n",
    "\n",
    "def extract_query_text(query_input):\n",
    "    if isinstance(query_input, list):\n",
    "        for item in query_input:\n",
    "            if isinstance(item, dict) and item.get(\"role\") == \"user\":\n",
    "                content = item.get(\"content\", [])\n",
    "                if isinstance(content, list) and len(content) > 0:\n",
    "                    return content[0].get(\"text\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_response_text(response):\n",
    "    if isinstance(response, list):\n",
    "        for item in response:\n",
    "            if isinstance(item, dict) and item.get(\"role\") == \"assistant\":\n",
    "                content = item.get(\"content\", [])\n",
    "                if isinstance(content, list) and len(content) > 0:\n",
    "                    return content[0].get(\"text\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "eval_output_path = Path(\"evals/eval-output.json\")\n",
    "\n",
    "print(LINE)\n",
    "print(\"ğŸ“Š AGENT EVALUATION RESULTS - ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸\")\n",
    "print(LINE, \"\\n\")\n",
    "\n",
    "if not eval_output_path.exists():\n",
    "    print(\"âŒ í‰ê°€ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"   íŒŒì¼ ê²½ë¡œ: {eval_output_path.absolute()}\")\n",
    "    print(\"\\n   ë¨¼ì € 06_evaluate_agents.ipynbì˜ ì…€ 5ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\\n\")\n",
    "else:\n",
    "    with open(eval_output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metrics = data.get(\"metrics\", {})\n",
    "    rows = data.get(\"rows\", [])\n",
    "    \n",
    "    # ì„¹ì…˜ 1: ì „ì²´ í‰ê·  ì ìˆ˜\n",
    "    print(\"â­ ì „ì²´ í‰ê·  ì„±ëŠ¥ ì ìˆ˜\")\n",
    "    print(LINE)\n",
    "    scores_config = [\n",
    "        (\"Intent Resolution\", \"intent_resolution.intent_resolution\", \"ì˜ë„ íŒŒì•…\", 3.0),\n",
    "        (\"Task Adherence\", \"task_adherence.task_adherence\", \"ì‘ì—… ì¶©ì‹¤ë„\", 3.0),\n",
    "    ]\n",
    "    \n",
    "    for name, key, desc, threshold in scores_config:\n",
    "        if key in metrics:\n",
    "            score = metrics[key]\n",
    "            color = get_score_color(score, threshold)\n",
    "            reset = reset_color()\n",
    "            indicator = get_score_indicator(score, threshold)\n",
    "            stars = \"â˜…\" * int(score) + \"â˜†\" * (5 - int(score))\n",
    "            bar = \"â–ˆ\" * int(score * 4) + \"â–‘\" * (20 - int(score * 4))\n",
    "            \n",
    "            print(f\"{indicator} {name:20} {color}{score:.2f}/5.0{reset}  {stars}\")\n",
    "            print(f\"     {desc:20} [{bar}]\")\n",
    "            if score < threshold:\n",
    "                print(f\"     {color}âš ï¸ ì„ê³„ê°’ ë¯¸ë‹¬ (ê¸°ì¤€: {threshold:.1f}){reset}\")\n",
    "            print()\n",
    "    \n",
    "    # ì„¹ì…˜ 2: ìš´ì˜ ë©”íŠ¸ë¦­\n",
    "    print(\"\\nâš¡ ìš´ì˜ ë©”íŠ¸ë¦­ (í‰ê· )\")\n",
    "    print(LINE)\n",
    "    \n",
    "    operational_keys = [\n",
    "        (\"operational_metrics.server-run-duration-in-seconds\", \"ì„œë²„ ì‹¤í–‰ ì‹œê°„\", \"s\"),\n",
    "        (\"operational_metrics.client-run-duration-in-seconds\", \"í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰ ì‹œê°„\", \"s\"),\n",
    "        (\"operational_metrics.prompt-tokens\", \"í”„ë¡¬í”„íŠ¸ í† í°\", \"tokens\"),\n",
    "        (\"operational_metrics.completion-tokens\", \"ì™„ì„± í† í°\", \"tokens\"),\n",
    "    ]\n",
    "    \n",
    "    total_tokens = 0\n",
    "    for key, desc, unit in operational_keys:\n",
    "        if key in metrics:\n",
    "            value = metrics[key]\n",
    "            if unit == \"tokens\":\n",
    "                print(f\"  {desc:30} {int(value):>10,} {unit}\")\n",
    "                total_tokens += value\n",
    "            else:\n",
    "                print(f\"  {desc:30} {value:>10.2f} {unit}\")\n",
    "    \n",
    "    if total_tokens > 0:\n",
    "        print(f\"  {'ì´ í† í° ì‚¬ìš©ëŸ‰':30} {int(total_tokens):>10,} tokens\")\n",
    "        cost = (total_tokens / 1000) * 0.0025\n",
    "        print(f\"  {'ì˜ˆìƒ ë¹„ìš© (GPT-4o)':30} ${cost:>9.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # ì„¹ì…˜ 3: ê°œë³„ ì¿¼ë¦¬ ìƒì„¸ ê²°ê³¼\n",
    "    if rows:\n",
    "        print(\"\\nğŸ“‹ ì¿¼ë¦¬ë³„ ìƒì„¸ ê²°ê³¼\")\n",
    "        print(LINE)\n",
    "        \n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            query = extract_query_text(row.get(\"inputs.query\", []))\n",
    "            response = extract_response_text(row.get(\"inputs.response\", []))\n",
    "            ground_truth = row.get(\"inputs.metrics.ground-truth\", \"\")\n",
    "            \n",
    "            print(f\"\\n{SEPARATOR}\")\n",
    "            print(f\"ğŸ” Query #{idx}\")\n",
    "            print(SEPARATOR)\n",
    "            \n",
    "            print(\"\\nğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸:\")\n",
    "            print(f\"   {query}\")\n",
    "            \n",
    "            if ground_truth:\n",
    "                print(\"\\nğŸ“Œ ì˜ˆìƒ ë™ì‘ (Ground Truth):\")\n",
    "                print(f\"   {ground_truth}\")\n",
    "            \n",
    "            if response:\n",
    "                print(\"\\nğŸ¤– Agent ì‘ë‹µ (ìš”ì•½):\")\n",
    "                response_preview = response[:200] if len(response) > 200 else response\n",
    "                lines_shown = 0\n",
    "                for line in response_preview.split(\"\\n\"):\n",
    "                    if line.strip() and lines_shown < 3:\n",
    "                        print(f\"   {line.strip()}\")\n",
    "                        lines_shown += 1\n",
    "                if len(response) > 200:\n",
    "                    print(f\"   ... (ì´ {len(response):,}ì)\")\n",
    "            \n",
    "            print(\"\\nğŸ“Š í‰ê°€ ì ìˆ˜:\")\n",
    "            \n",
    "            intent = row.get(\"outputs.intent_resolution.intent_resolution\", \"N/A\")\n",
    "            task = row.get(\"outputs.task_adherence.task_adherence\", \"N/A\")\n",
    "            tool = row.get(\"outputs.tool_call_accuracy.tool_call_accuracy\", \"N/A\")\n",
    "            \n",
    "            intent_threshold = row.get(\"outputs.intent_resolution.intent_resolution_threshold\", 3)\n",
    "            task_threshold = row.get(\"outputs.task_adherence.task_adherence_threshold\", 3)\n",
    "            \n",
    "            if isinstance(intent, (int, float)):\n",
    "                color = get_score_color(intent, intent_threshold)\n",
    "                reset = reset_color()\n",
    "                indicator = get_score_indicator(intent, intent_threshold)\n",
    "                print(f\"   {indicator} Intent Resolution:  {color}{intent:.1f}/5.0{reset} (ì„ê³„ê°’: {intent_threshold})\")\n",
    "            else:\n",
    "                print(f\"   â€¢ Intent Resolution:  {intent}\")\n",
    "            \n",
    "            if isinstance(task, (int, float)):\n",
    "                color = get_score_color(task, task_threshold)\n",
    "                reset = reset_color()\n",
    "                indicator = get_score_indicator(task, task_threshold)\n",
    "                print(f\"   {indicator} Task Adherence:     {color}{task:.1f}/5.0{reset} (ì„ê³„ê°’: {task_threshold})\")\n",
    "            else:\n",
    "                print(f\"   â€¢ Task Adherence:     {task}\")\n",
    "            \n",
    "            print(f\"   â€¢ Tool Call Accuracy: {tool}\")\n",
    "            \n",
    "            # í‰ê°€ ì´ìœ \n",
    "            print(\"\\nğŸ’¡ í‰ê°€ ìƒì„¸:\")\n",
    "            \n",
    "            intent_reason = row.get(\"outputs.intent_resolution.intent_resolution_reason\", \"\")\n",
    "            task_reason = row.get(\"outputs.task_adherence.task_adherence_reason\", \"\")\n",
    "            tool_reason = row.get(\"outputs.tool_call_accuracy.tool_call_accuracy_reason\", \"\")\n",
    "            \n",
    "            if intent_reason:\n",
    "                print(\"\\n   [Intent Resolution í‰ê°€ ì´ìœ ]\")\n",
    "                for sentence in intent_reason.split(\". \"):\n",
    "                    if sentence.strip():\n",
    "                        print(f\"   â€¢ {sentence.strip()}.\")\n",
    "            \n",
    "            if task_reason:\n",
    "                print(\"\\n   [Task Adherence í‰ê°€ ì´ìœ ]\")\n",
    "                for sentence in task_reason.split(\". \"):\n",
    "                    if sentence.strip():\n",
    "                        print(f\"   â€¢ {sentence.strip()}.\")\n",
    "            \n",
    "            if tool_reason:\n",
    "                print(\"\\n   [Tool Call Accuracy í‰ê°€ ì´ìœ ]\")\n",
    "                for sentence in tool_reason.split(\". \"):\n",
    "                    if sentence.strip():\n",
    "                        print(f\"   â€¢ {sentence.strip()}.\")\n",
    "            \n",
    "            duration = row.get(\"outputs.operational_metrics.client-run-duration-in-seconds\", 0)\n",
    "            prompt_tokens = row.get(\"outputs.operational_metrics.prompt-tokens\", 0)\n",
    "            completion_tokens = row.get(\"outputs.operational_metrics.completion-tokens\", 0)\n",
    "            \n",
    "            print(\"\\nâ±ï¸  ì„±ëŠ¥ ë©”íŠ¸ë¦­:\")\n",
    "            print(f\"   â€¢ ì‹¤í–‰ ì‹œê°„: {duration:.2f}ì´ˆ\")\n",
    "            print(f\"   â€¢ í† í° ì‚¬ìš©: {prompt_tokens:,} (ì…ë ¥) + {completion_tokens:,} (ì¶œë ¥) = {prompt_tokens + completion_tokens:,} (ì´)\")\n",
    "            \n",
    "            issues = []\n",
    "            if isinstance(intent, (int, float)) and intent < intent_threshold:\n",
    "                issues.append(f\"Intent Resolution ì ìˆ˜ ë‚®ìŒ ({intent:.1f} < {intent_threshold})\")\n",
    "            if isinstance(task, (int, float)) and task < task_threshold:\n",
    "                issues.append(f\"Task Adherence ì ìˆ˜ ë‚®ìŒ ({task:.1f} < {task_threshold})\")\n",
    "            \n",
    "            if issues:\n",
    "                print(f\"\\n{get_score_color(1.0, 3.0)}âš ï¸  ë°œê²¬ëœ ë¬¸ì œ:{reset_color()}\")\n",
    "                for issue in issues:\n",
    "                    print(f\"   â€¢ {issue}\")\n",
    "        \n",
    "        # ì„¹ì…˜ 4: í†µê³„ ìš”ì•½\n",
    "        print(f\"\\n{SEPARATOR}\\n\")\n",
    "        print(\"\\nğŸ“ˆ í†µê³„ ìš”ì•½ ë° ë¶„ì„\")\n",
    "        print(LINE)\n",
    "        \n",
    "        intent_scores = []\n",
    "        task_scores = []\n",
    "        durations = []\n",
    "        total_tokens_list = []\n",
    "        failed_queries = []\n",
    "        \n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            intent = row.get(\"outputs.intent_resolution.intent_resolution\")\n",
    "            task = row.get(\"outputs.task_adherence.task_adherence\")\n",
    "            duration = row.get(\"outputs.operational_metrics.client-run-duration-in-seconds\", 0)\n",
    "            prompt = row.get(\"outputs.operational_metrics.prompt-tokens\", 0)\n",
    "            completion = row.get(\"outputs.operational_metrics.completion-tokens\", 0)\n",
    "            \n",
    "            intent_threshold = row.get(\"outputs.intent_resolution.intent_resolution_threshold\", 3)\n",
    "            task_threshold = row.get(\"outputs.task_adherence.task_adherence_threshold\", 3)\n",
    "            \n",
    "            if isinstance(intent, (int, float)):\n",
    "                intent_scores.append(intent)\n",
    "                if intent < intent_threshold:\n",
    "                    query = extract_query_text(row.get(\"inputs.query\", []))\n",
    "                    failed_queries.append((idx, \"Intent Resolution\", intent, query[:50]))\n",
    "            \n",
    "            if isinstance(task, (int, float)):\n",
    "                task_scores.append(task)\n",
    "                if task < task_threshold:\n",
    "                    query = extract_query_text(row.get(\"inputs.query\", []))\n",
    "                    failed_queries.append((idx, \"Task Adherence\", task, query[:50]))\n",
    "            \n",
    "            if duration:\n",
    "                durations.append(duration)\n",
    "            total_tokens_list.append(prompt + completion)\n",
    "        \n",
    "        if intent_scores:\n",
    "            avg_intent = sum(intent_scores) / len(intent_scores)\n",
    "            color = get_score_color(avg_intent, 3.0)\n",
    "            reset = reset_color()\n",
    "            pass_count = len([s for s in intent_scores if s >= 3.0])\n",
    "            \n",
    "            print(\"\\nğŸ“Š Intent Resolution (ì˜ë„ íŒŒì•…)\")\n",
    "            print(f\"   í‰ê· : {color}{avg_intent:.2f}/5.0{reset}\")\n",
    "            print(f\"   ìµœê³ : {max(intent_scores):.1f}  |  ìµœì €: {min(intent_scores):.1f}\")\n",
    "            print(f\"   í•©ê²©ë¥ : {pass_count}/{len(intent_scores)} ({pass_count/len(intent_scores)*100:.1f}%)\")\n",
    "        \n",
    "        if task_scores:\n",
    "            avg_task = sum(task_scores) / len(task_scores)\n",
    "            color = get_score_color(avg_task, 3.0)\n",
    "            reset = reset_color()\n",
    "            pass_count = len([s for s in task_scores if s >= 3.0])\n",
    "            \n",
    "            print(\"\\nğŸ“Š Task Adherence (ì‘ì—… ì¶©ì‹¤ë„)\")\n",
    "            print(f\"   í‰ê· : {color}{avg_task:.2f}/5.0{reset}\")\n",
    "            print(f\"   ìµœê³ : {max(task_scores):.1f}  |  ìµœì €: {min(task_scores):.1f}\")\n",
    "            print(f\"   í•©ê²©ë¥ : {pass_count}/{len(task_scores)} ({pass_count/len(task_scores)*100:.1f}%)\")\n",
    "        \n",
    "        if durations:\n",
    "            print(\"\\nâ±ï¸  ì‹¤í–‰ ì‹œê°„\")\n",
    "            print(f\"   í‰ê· : {sum(durations)/len(durations):.2f}ì´ˆ\")\n",
    "            print(f\"   ìµœëŒ€: {max(durations):.2f}ì´ˆ  |  ìµœì†Œ: {min(durations):.2f}ì´ˆ\")\n",
    "        \n",
    "        if total_tokens_list:\n",
    "            avg_tokens = sum(total_tokens_list) / len(total_tokens_list)\n",
    "            total_all_tokens = sum(total_tokens_list)\n",
    "            \n",
    "            print(\"\\nğŸ’° í† í° ì‚¬ìš©ëŸ‰\")\n",
    "            print(f\"   í‰ê· : {avg_tokens:,.0f} tokens/query\")\n",
    "            print(f\"   ì´í•©: {total_all_tokens:,} tokens\")\n",
    "            print(f\"   ìµœëŒ€: {max(total_tokens_list):,}  |  ìµœì†Œ: {min(total_tokens_list):,}\")\n",
    "            print(f\"   ì˜ˆìƒ ë¹„ìš© (GPT-4o): ${(total_all_tokens / 1000) * 0.0025:.4f}\")\n",
    "        \n",
    "        if failed_queries:\n",
    "            print(f\"\\n{get_score_color(1.0, 3.0)}âš ï¸  ê°œì„ ì´ í•„ìš”í•œ ì¿¼ë¦¬ ({len(failed_queries)}ê°œ){reset_color()}\")\n",
    "            print(SEPARATOR)\n",
    "            \n",
    "            seen = set()\n",
    "            for idx, metric, score, query in failed_queries:\n",
    "                key = (idx, metric)\n",
    "                if key not in seen:\n",
    "                    seen.add(key)\n",
    "                    print(f\"   Query #{idx}: {metric} = {score:.1f}\")\n",
    "                    print(f\"   â””â”€ {query}...\")\n",
    "                    print()\n",
    "        else:\n",
    "            print(\"\\nâœ… ëª¨ë“  ì¿¼ë¦¬ê°€ ì„ê³„ê°’ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "    print(f\"\\n{LINE}\")\n",
    "    print(f\"âœ… ì´ {len(rows)}ê°œ ì¿¼ë¦¬ í‰ê°€ ì™„ë£Œ\")\n",
    "    print(f\"ğŸ“ ìƒì„¸ JSON: {eval_output_path.absolute()}\")\n",
    "    print(f\"ğŸ’¡ í„°ë¯¸ë„ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥: python3 show_eval_results.py\")\n",
    "    print(f\"{LINE}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4900276",
   "metadata": {},
   "source": [
    "## 7. Evaluation ë©”íŠ¸ë¦­ í•´ì„ ê°€ì´ë“œ\n",
    "\n",
    "### ë©”íŠ¸ë¦­ í•´ì„\n",
    "\n",
    "**Operational Metrics:**\n",
    "- `server-run-duration-in-seconds`: ì„œë²„ì—ì„œ Agent ì‹¤í–‰ ì‹œê°„\n",
    "- `client-run-duration-in-seconds`: í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì¸¡ì •í•œ ì´ ì†Œìš” ì‹œê°„\n",
    "- `prompt-tokens`: ì…ë ¥ í† í° ìˆ˜\n",
    "- `completion-tokens`: ìƒì„±ëœ í† í° ìˆ˜\n",
    "\n",
    "**Performance Metrics:**\n",
    "- `tool_call_accuracy.*`: ë„êµ¬ í˜¸ì¶œ ì •í™•ë„ (1-5ì )\n",
    "- `intent_resolution.*`: ì˜ë„ íŒŒì•… ì •í™•ë„ (1-5ì )\n",
    "- `task_adherence.*`: ì‘ì—… ì¤€ìˆ˜ë„ (1-5ì )\n",
    "\n",
    "**ì ìˆ˜ í•´ì„:**\n",
    "- 5ì : ì™„ë²½í•¨\n",
    "- 4ì : ì¢‹ìŒ\n",
    "- 3ì : ë³´í†µ\n",
    "- 2ì : ê°œì„  í•„ìš”\n",
    "- 1ì : ë§¤ìš° ë‚˜ì¨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1fd41",
   "metadata": {},
   "source": [
    "## 8. ë‹¤ìŒ ë‹¨ê³„ ë° Agent ê°œì„ \n",
    "\n",
    "### Agent ê°œì„  ë°©ë²•\n",
    "\n",
    "1. **ë‚®ì€ ì ìˆ˜ ë¶„ì„**\n",
    "   - ì–´ë–¤ ì¿¼ë¦¬ì—ì„œ ì ìˆ˜ê°€ ë‚®ì•˜ëŠ”ì§€ í™•ì¸\n",
    "   - ì–´ë–¤ evaluatorì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆëŠ”ì§€ íŒŒì•…\n",
    "\n",
    "2. **í”„ë¡¬í”„íŠ¸ ê°œì„ **\n",
    "   - Agent instructionsë¥¼ ë” ëª…í™•í•˜ê²Œ ì‘ì„±\n",
    "   - ì˜ˆì‹œ ì¶”ê°€\n",
    "   - ì œì•½ì‚¬í•­ ëª…ì‹œ\n",
    "\n",
    "3. **ê¸°ëŠ¥ ê°œì„ **\n",
    "   - Tool ì¶”ê°€ ë˜ëŠ” ê°œì„ \n",
    "   - RAG ì§€ì‹ ë² ì´ìŠ¤ ë³´ê°•\n",
    "   - Multi-Agent ì¡°ì •\n",
    "\n",
    "4. **ì¬í‰ê°€**\n",
    "   - ë™ì¼í•œ ì¿¼ë¦¬ë¡œ ì¬í‰ê°€\n",
    "   - ì ìˆ˜ ë³€í™” ë¹„êµ\n",
    "   - ì§€ì†ì ì¸ ê°œì„ \n",
    "\n",
    "### ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [Azure AI Foundry Agent Evaluation](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)\n",
    "- [Built-in Evaluators](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk)\n",
    "- [Evaluation Best Practices](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-approach-gen-ai)\n",
    "\n",
    "---\n",
    "\n",
    "## ì™„ë£Œ!\n",
    "\n",
    "ì¶•í•˜í•©ë‹ˆë‹¤! Agent Evaluationì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

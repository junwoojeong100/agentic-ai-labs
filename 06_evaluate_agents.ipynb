{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eea5182",
   "metadata": {},
   "source": [
    "# 06. Agent Evaluation (Agent 평가)\n",
    "\n",
    "이 노트북에서는 배포된 Agent의 품질과 성능을 평가합니다.\n",
    "\n",
    "## 목표\n",
    "\n",
    "- Azure AI Evaluation SDK를 사용한 Agent 평가\n",
    "- 성능, 정확도, 안전성 메트릭 수집\n",
    "- 평가 결과 분석 및 시각화\n",
    "\n",
    "## 사전 요구사항\n",
    "\n",
    "1. Notebook 01-03 완료 (Azure 리소스 및 Agent 배포)\n",
    "2. `config.json` 파일 존재\n",
    "3. Azure AI Project 접근 권한\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b3082",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⚙️ 시작하기 전에 (Before You Start)\n",
    "\n",
    "**Python 커널을 선택하세요:**\n",
    "\n",
    "1. 노트북 오른쪽 상단의 **\"커널 선택 (Select Kernel)\"** 클릭\n",
    "2. **\"Python Environments...\"** 선택\n",
    "3. **`.venv (Python 3.x.x)`** 선택 (프로젝트 루트에 생성된 가상환경)\n",
    "\n",
    "> 💡 **GitHub Codespaces**: Codespaces에서는 자동으로 `.venv` 환경이 생성됩니다.  \n",
    "> 만약 `.venv`가 보이지 않으면 터미널에서 `python -m venv .venv`로 생성하세요.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784224b9",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 Config 로드\n",
    "\n",
    "이전 노트북에서 생성한 설정을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eebcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# config.json 로드\n",
    "config_path = Path(\"config.json\")\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(\"config.json not found. Please run notebooks 01-03 first.\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# 주요 변수 추출\n",
    "PROJECT_CONNECTION_STRING = config.get(\"project_connection_string\", \"\")\n",
    "simple_project_conn = PROJECT_CONNECTION_STRING.split(';')[0] if PROJECT_CONNECTION_STRING else \"\"\n",
    "\n",
    "print(\"=== Configuration Loaded ===\")\n",
    "print(f\"Resource Group: {config.get('resource_group')}\")\n",
    "print(f\"Location: {config.get('location')}\")\n",
    "print(f\"Project: {simple_project_conn}\")\n",
    "print(f\"Model: {config.get('model_deployment_name')}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc8337",
   "metadata": {},
   "source": [
    "## 2. Azure AI Evaluation 패키지 설치\n",
    "\n",
    "Agent 평가에 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Evaluation 패키지 설치\n",
    "print(\"=== Installing Azure AI Evaluation Package ===\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"azure-ai-evaluation\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ azure-ai-evaluation installed successfully\")\n",
    "else:\n",
    "    print(f\"⚠️  Installation warning: {result.stderr}\")\n",
    "    print(\"   Proceeding anyway...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b155d",
   "metadata": {},
   "source": [
    "## 3. Evaluation 개요\n",
    "\n",
    "### Evaluation 유형\n",
    "\n",
    "**Performance Evaluators (성능 평가):**\n",
    "- **Intent Resolution**: Agent가 사용자 의도를 올바르게 파악했는지 평가\n",
    "- **Tool Call Accuracy**: Agent가 올바른 도구를 정확하게 호출했는지 평가  \n",
    "- **Task Adherence**: Agent가 지시사항을 충실히 따랐는지 평가\n",
    "\n",
    "**Safety Evaluators (안전성 평가):**\n",
    "- **Content Safety**: 부적절한 콘텐츠(폭력, 혐오 등) 포함 여부 평가\n",
    "- **Indirect Attack**: 간접적인 악의적 공격 시도 감지\n",
    "- **Code Vulnerability**: 생성된 코드의 보안 취약점 평가\n",
    "\n",
    "### Evaluation 프로세스\n",
    "\n",
    "1. 테스트 쿼리 생성\n",
    "2. Agent 생성 및 쿼리 실행\n",
    "3. 응답 수집 및 메트릭 측정\n",
    "4. Evaluator로 품질 평가\n",
    "5. 결과 저장 및 분석\n",
    "6. Agent 정리\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4deab",
   "metadata": {},
   "source": [
    "## 4. 테스트 쿼리 생성\n",
    "\n",
    "Agent의 다양한 기능을 테스트할 쿼리를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evals 디렉토리 생성\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "evals_dir = Path(\"evals\")\n",
    "evals_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=== Creating Evaluation Test Queries ===\\n\")\n",
    "\n",
    "# 테스트 쿼리 정의\n",
    "eval_queries = [\n",
    "    {\n",
    "        \"query\": \"안녕하세요. 가족 여행지를 추천받고 싶어요.\",\n",
    "        \"ground-truth\": \"Agent는 인사에 응답하고, 여행지 추천을 위해 Research Agent를 호출해야 합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"부산의 현재 날씨를 알려주세요. 온도와 체감온도를 포함해주세요.\",\n",
    "        \"ground-truth\": \"부산의 현재 날씨 정보를 정확하게 제공해야 하며, 온도와 체감온도를 모두 포함해야 합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"제주도에서 가족과 함께 즐길 수 있는 여행지는 어디인가요?\",\n",
    "        \"ground-truth\": \"제주도의 가족 친화적인 여행지를 검색하여 추천해야 합니다. 자연 명소, 체험 활동, 접근성 등을 고려한 정보를 제공합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"서핑할 수 있는 해변을 추천해주세요.\",\n",
    "        \"ground-truth\": \"서핑이 가능한 한국의 해변 여행지를 검색하여 추천해야 합니다. 양양, 부산 등의 서핑 명소 정보를 제공합니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"가을에 가기 좋은 단풍 명소는 어디인가요?\",\n",
    "        \"ground-truth\": \"가을 계절에 방문하기 좋은 단풍 명소를 검색하여 추천해야 합니다. 내장산, 설악산 등의 자연 명소 정보를 제공합니다.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# JSON 파일로 저장\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "with open(eval_queries_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Created {eval_queries_path}\")\n",
    "print(f\"\\n📋 Test Queries ({len(eval_queries)} total):\\n\")\n",
    "\n",
    "for i, query in enumerate(eval_queries, 1):\n",
    "    print(f\"   {i}. {query['query'][:60]}...\")\n",
    "\n",
    "print(\"\\n💡 각 쿼리는 Agent의 다른 기능을 테스트합니다:\")\n",
    "print(\"   • 일반 대화 및 여행 의도 파악\")\n",
    "print(\"   • 날씨 조회 (Tool 기능)\")\n",
    "print(\"   • 여행지 지식 검색 (RAG 기능)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded78b6",
   "metadata": {},
   "source": [
    "## 5. Agent Evaluation 실행\n",
    "\n",
    "평가용 Agent를 생성하고 테스트 쿼리를 실행한 후, Evaluator로 품질을 평가합니다.\n",
    "\n",
    "### 참고사항\n",
    "\n",
    "- **실행 시간**: 약 2-3분 소요\n",
    "- **리전 제약**: eastus 리전에서는 일부 Safety Evaluator가 지원되지 않습니다\n",
    "- **권한 이슈**: Azure AI Project의 스토리지 권한이 필요할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Evaluation 실행\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from azure.ai.agents.models import RunStatus, MessageRole\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.evaluation import (\n",
    "    AIAgentConverter, evaluate, ToolCallAccuracyEvaluator, IntentResolutionEvaluator, \n",
    "    TaskAdherenceEvaluator\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"=== Running Agent Evaluation ===\\n\")\n",
    "\n",
    "# 파일 경로 설정\n",
    "current_dir = Path(\".\")\n",
    "evals_dir = current_dir / \"evals\"\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "eval_input_path = evals_dir / \"eval-input.jsonl\"\n",
    "eval_output_path = evals_dir / \"eval-output.json\"\n",
    "\n",
    "# 환경 변수 로드 (이미 config에서 로드됨)\n",
    "project_endpoint = simple_project_conn\n",
    "parsed_endpoint = urlparse(project_endpoint)\n",
    "model_endpoint = f\"{parsed_endpoint.scheme}://{parsed_endpoint.netloc}\"\n",
    "deployment_name = config.get(\"model_deployment_name\", \"gpt-4o\")\n",
    "\n",
    "print(f\"📋 Configuration:\")\n",
    "print(f\"   Project: {project_endpoint}\")\n",
    "print(f\"   Model: {deployment_name}\")\n",
    "print(f\"   Test Queries: {eval_queries_path}\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Initialize AIProjectClient\n",
    "print(\"🔌 Connecting to AI Project...\")\n",
    "credential = DefaultAzureCredential()\n",
    "ai_project = AIProjectClient(\n",
    "    credential=credential,\n",
    "    endpoint=project_endpoint,\n",
    "    api_version=\"2025-05-15-preview\"  # Evaluations require preview API\n",
    ")\n",
    "print(\"✅ Connected\\n\")\n",
    "\n",
    "# Evaluation용 Agent 생성\n",
    "print(\"🤖 Creating Evaluation Agent...\")\n",
    "eval_agent = ai_project.agents.create_agent(\n",
    "    model=deployment_name,\n",
    "    name=\"Evaluation Agent\",\n",
    "    instructions=\"\"\"You are a helpful travel and weather assistant.\n",
    "    \n",
    "You can help users with:\n",
    "1. Travel recommendations and destination information\n",
    "2. Weather information for any city\n",
    "3. General travel planning advice\n",
    "\n",
    "Be friendly, informative, and provide detailed responses.\"\"\"\n",
    ")\n",
    "print(f\"✅ Agent created: {eval_agent.name} (ID: {eval_agent.id})\\n\")\n",
    "\n",
    "# Setup evaluation config\n",
    "api_version = config.get(\"api_version\", \"2024-08-01-preview\")\n",
    "model_config = {\n",
    "    \"azure_deployment\": deployment_name,\n",
    "    \"azure_endpoint\": model_endpoint,\n",
    "    \"api_version\": api_version,\n",
    "}\n",
    "\n",
    "thread_data_converter = AIAgentConverter(ai_project)\n",
    "\n",
    "# 테스트 쿼리 실행 및 evaluation input 준비\n",
    "print(\"=\"*70)\n",
    "print(\"📝 Executing Test Queries\\n\")\n",
    "\n",
    "with open(eval_queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"   Total queries: {len(test_data)}\\n\")\n",
    "\n",
    "with open(eval_input_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, row in enumerate(test_data, 1):\n",
    "        query_text = row.get(\"query\")\n",
    "        print(f\"   [{idx}/{len(test_data)}] {query_text[:50]}...\")\n",
    "        \n",
    "        # 새 스레드 생성 (각 쿼리를 격리)\n",
    "        thread = ai_project.agents.threads.create()\n",
    "        \n",
    "        # 사용자 쿼리 생성\n",
    "        ai_project.agents.messages.create(\n",
    "            thread.id, role=MessageRole.USER, content=query_text\n",
    "        )\n",
    "        \n",
    "        # Agent 실행 및 성능 측정\n",
    "        start_time = time.time()\n",
    "        run = ai_project.agents.runs.create_and_process(\n",
    "            thread_id=thread.id, agent_id=eval_agent.id\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if run.status != RunStatus.COMPLETED:\n",
    "            print(f\"      ⚠️  Run failed: {run.last_error or 'Unknown error'}\")\n",
    "            continue\n",
    "        \n",
    "        # 운영 메트릭 수집\n",
    "        operational_metrics = {\n",
    "            \"server-run-duration-in-seconds\": (\n",
    "                run.completed_at - run.created_at\n",
    "            ).total_seconds(),\n",
    "            \"client-run-duration-in-seconds\": end_time - start_time,\n",
    "            \"completion-tokens\": run.usage.completion_tokens,\n",
    "            \"prompt-tokens\": run.usage.prompt_tokens,\n",
    "            \"ground-truth\": row.get(\"ground-truth\", '')\n",
    "        }\n",
    "        \n",
    "        # Thread 데이터 + 운영 메트릭을 evaluation input에 추가\n",
    "        evaluation_data = thread_data_converter.prepare_evaluation_data(thread_ids=thread.id)\n",
    "        eval_item = evaluation_data[0]\n",
    "        eval_item[\"metrics\"] = operational_metrics\n",
    "        f.write(json.dumps(eval_item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"      ✅ Completed in {operational_metrics['client-run-duration-in-seconds']:.1f}s\")\n",
    "        print(f\"         Tokens: {operational_metrics['prompt-tokens']} prompt + {operational_metrics['completion-tokens']} completion\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ All queries executed successfully\")\n",
    "print(f\"   Input saved to: {eval_input_path}\\n\")\n",
    "\n",
    "# Evaluation 실행\n",
    "print(\"=\"*70)\n",
    "print(\"🔬 Running Evaluators\\n\")\n",
    "\n",
    "print(\"   Evaluators:\")\n",
    "print(\"      • ToolCallAccuracyEvaluator\")\n",
    "print(\"      • IntentResolutionEvaluator\")\n",
    "print(\"      • TaskAdherenceEvaluator\")\n",
    "print(\"\\n   ⚠️  참고: eastus 리전에서는 일부 RAI 평가자가 지원되지 않습니다.\")\n",
    "print(\"      (CodeVulnerability, ContentSafety, IndirectAttack는 제외)\\n\")\n",
    "print(\"   ⏳ This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# OperationalMetricsEvaluator 정의\n",
    "class OperationalMetricsEvaluator:\n",
    "    \"\"\"Propagate operational metrics to the final evaluation results\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, *, metrics: dict, **kwargs):\n",
    "        return metrics\n",
    "\n",
    "# Evaluation 실행 (로컬)\n",
    "print(\"   💡 평가 결과를 로컬에 저장합니다.\\n\")\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"foundry-agent-evaluation\",\n",
    "    data=eval_input_path,\n",
    "    evaluators={\n",
    "        \"operational_metrics\": OperationalMetricsEvaluator(),\n",
    "        \"tool_call_accuracy\": ToolCallAccuracyEvaluator(model_config=model_config),\n",
    "        \"intent_resolution\": IntentResolutionEvaluator(model_config=model_config),\n",
    "        \"task_adherence\": TaskAdherenceEvaluator(model_config=model_config),\n",
    "        # eastus 리전에서 지원되지 않는 평가자들은 제외\n",
    "        # \"code_vulnerability\": CodeVulnerabilityEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"content_safety\": ContentSafetyEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"indirect_attack\": IndirectAttackEvaluator(credential=credential, azure_ai_project=project_endpoint)\n",
    "    },\n",
    "    output_path=eval_output_path,\n",
    "    # azure_ai_project 파라미터 제거 (ML workspace 불필요)\n",
    ")\n",
    "\n",
    "print(\"✅ Evaluation completed!\\n\")\n",
    "print(f\"📁 결과 저장 위치: {eval_output_path}\")\n",
    "print(f\"   다음 셀에서 결과를 확인하세요.\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluation Agent 삭제\n",
    "print(\"🧹 Cleaning up...\")\n",
    "ai_project.agents.delete_agent(eval_agent.id)\n",
    "print(f\"✅ Evaluation Agent deleted: {eval_agent.id}\\n\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dbb91",
   "metadata": {},
   "source": [
    "## 6. Evaluation 결과 시각화\n",
    "\n",
    "결과를 표와 차트로 시각화하여 분석합니다.\n",
    "\n",
    "### 결과 확인 방법\n",
    "\n",
    "**방법 1: 노트북에서 확인**\n",
    "- 아래 셀을 실행하여 노트북에서 바로 결과를 확인합니다.\n",
    "\n",
    "**방법 2: 터미널에서 확인**\n",
    "- 터미널에서 다음 명령어를 실행하여 동일한 결과를 확인할 수 있습니다:\n",
    "  ```bash\n",
    "  python3 show_eval_results.py\n",
    "  ```\n",
    "- 이 스크립트는 프로젝트 루트에 자동으로 생성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation 결과 상세 시각화\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "SEPARATOR = \"─\" * 100\n",
    "LINE = \"=\" * 100\n",
    "\n",
    "def get_score_color(score, threshold=3.0):\n",
    "    if score >= 4.5:\n",
    "        return \"\\033[92m\"\n",
    "    elif score >= threshold:\n",
    "        return \"\\033[93m\"\n",
    "    else:\n",
    "        return \"\\033[91m\"\n",
    "\n",
    "def reset_color():\n",
    "    return \"\\033[0m\"\n",
    "\n",
    "def get_score_indicator(score, threshold=3.0):\n",
    "    if score >= 4.5:\n",
    "        return \"✅\"\n",
    "    elif score >= threshold:\n",
    "        return \"⚠️\"\n",
    "    else:\n",
    "        return \"❌\"\n",
    "\n",
    "def extract_query_text(query_input):\n",
    "    if isinstance(query_input, list):\n",
    "        for item in query_input:\n",
    "            if isinstance(item, dict) and item.get(\"role\") == \"user\":\n",
    "                content = item.get(\"content\", [])\n",
    "                if isinstance(content, list) and len(content) > 0:\n",
    "                    return content[0].get(\"text\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_response_text(response):\n",
    "    if isinstance(response, list):\n",
    "        for item in response:\n",
    "            if isinstance(item, dict) and item.get(\"role\") == \"assistant\":\n",
    "                content = item.get(\"content\", [])\n",
    "                if isinstance(content, list) and len(content) > 0:\n",
    "                    return content[0].get(\"text\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "eval_output_path = Path(\"evals/eval-output.json\")\n",
    "\n",
    "print(LINE)\n",
    "print(\"📊 AGENT EVALUATION RESULTS - 상세 분석 리포트\")\n",
    "print(LINE, \"\\n\")\n",
    "\n",
    "if not eval_output_path.exists():\n",
    "    print(\"❌ 평가 결과 파일이 없습니다.\")\n",
    "    print(f\"   파일 경로: {eval_output_path.absolute()}\")\n",
    "    print(\"\\n   먼저 06_evaluate_agents.ipynb의 셀 5를 실행하세요.\\n\")\n",
    "else:\n",
    "    with open(eval_output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    metrics = data.get(\"metrics\", {})\n",
    "    rows = data.get(\"rows\", [])\n",
    "    \n",
    "    # 섹션 1: 전체 평균 점수\n",
    "    print(\"⭐ 전체 평균 성능 점수\")\n",
    "    print(LINE)\n",
    "    scores_config = [\n",
    "        (\"Intent Resolution\", \"intent_resolution.intent_resolution\", \"의도 파악\", 3.0),\n",
    "        (\"Task Adherence\", \"task_adherence.task_adherence\", \"작업 충실도\", 3.0),\n",
    "    ]\n",
    "    \n",
    "    for name, key, desc, threshold in scores_config:\n",
    "        if key in metrics:\n",
    "            score = metrics[key]\n",
    "            color = get_score_color(score, threshold)\n",
    "            reset = reset_color()\n",
    "            indicator = get_score_indicator(score, threshold)\n",
    "            stars = \"★\" * int(score) + \"☆\" * (5 - int(score))\n",
    "            bar = \"█\" * int(score * 4) + \"░\" * (20 - int(score * 4))\n",
    "            \n",
    "            print(f\"{indicator} {name:20} {color}{score:.2f}/5.0{reset}  {stars}\")\n",
    "            print(f\"     {desc:20} [{bar}]\")\n",
    "            if score < threshold:\n",
    "                print(f\"     {color}⚠️ 임계값 미달 (기준: {threshold:.1f}){reset}\")\n",
    "            print()\n",
    "    \n",
    "    # 섹션 2: 운영 메트릭\n",
    "    print(\"\\n⚡ 운영 메트릭 (평균)\")\n",
    "    print(LINE)\n",
    "    \n",
    "    operational_keys = [\n",
    "        (\"operational_metrics.server-run-duration-in-seconds\", \"서버 실행 시간\", \"s\"),\n",
    "        (\"operational_metrics.client-run-duration-in-seconds\", \"클라이언트 실행 시간\", \"s\"),\n",
    "        (\"operational_metrics.prompt-tokens\", \"프롬프트 토큰\", \"tokens\"),\n",
    "        (\"operational_metrics.completion-tokens\", \"완성 토큰\", \"tokens\"),\n",
    "    ]\n",
    "    \n",
    "    total_tokens = 0\n",
    "    for key, desc, unit in operational_keys:\n",
    "        if key in metrics:\n",
    "            value = metrics[key]\n",
    "            if unit == \"tokens\":\n",
    "                print(f\"  {desc:30} {int(value):>10,} {unit}\")\n",
    "                total_tokens += value\n",
    "            else:\n",
    "                print(f\"  {desc:30} {value:>10.2f} {unit}\")\n",
    "    \n",
    "    if total_tokens > 0:\n",
    "        print(f\"  {'총 토큰 사용량':30} {int(total_tokens):>10,} tokens\")\n",
    "        cost = (total_tokens / 1000) * 0.0025\n",
    "        print(f\"  {'예상 비용 (GPT-4o)':30} ${cost:>9.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # 섹션 3: 개별 쿼리 상세 결과\n",
    "    if rows:\n",
    "        print(\"\\n📋 쿼리별 상세 결과\")\n",
    "        print(LINE)\n",
    "        \n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            query = extract_query_text(row.get(\"inputs.query\", []))\n",
    "            response = extract_response_text(row.get(\"inputs.response\", []))\n",
    "            ground_truth = row.get(\"inputs.metrics.ground-truth\", \"\")\n",
    "            \n",
    "            print(f\"\\n{SEPARATOR}\")\n",
    "            print(f\"🔍 Query #{idx}\")\n",
    "            print(SEPARATOR)\n",
    "            \n",
    "            print(\"\\n💬 사용자 질문:\")\n",
    "            print(f\"   {query}\")\n",
    "            \n",
    "            if ground_truth:\n",
    "                print(\"\\n📌 예상 동작 (Ground Truth):\")\n",
    "                print(f\"   {ground_truth}\")\n",
    "            \n",
    "            if response:\n",
    "                print(\"\\n🤖 Agent 응답 (요약):\")\n",
    "                response_preview = response[:200] if len(response) > 200 else response\n",
    "                lines_shown = 0\n",
    "                for line in response_preview.split(\"\\n\"):\n",
    "                    if line.strip() and lines_shown < 3:\n",
    "                        print(f\"   {line.strip()}\")\n",
    "                        lines_shown += 1\n",
    "                if len(response) > 200:\n",
    "                    print(f\"   ... (총 {len(response):,}자)\")\n",
    "            \n",
    "            print(\"\\n📊 평가 점수:\")\n",
    "            \n",
    "            intent = row.get(\"outputs.intent_resolution.intent_resolution\", \"N/A\")\n",
    "            task = row.get(\"outputs.task_adherence.task_adherence\", \"N/A\")\n",
    "            tool = row.get(\"outputs.tool_call_accuracy.tool_call_accuracy\", \"N/A\")\n",
    "            \n",
    "            intent_threshold = row.get(\"outputs.intent_resolution.intent_resolution_threshold\", 3)\n",
    "            task_threshold = row.get(\"outputs.task_adherence.task_adherence_threshold\", 3)\n",
    "            \n",
    "            if isinstance(intent, (int, float)):\n",
    "                color = get_score_color(intent, intent_threshold)\n",
    "                reset = reset_color()\n",
    "                indicator = get_score_indicator(intent, intent_threshold)\n",
    "                print(f\"   {indicator} Intent Resolution:  {color}{intent:.1f}/5.0{reset} (임계값: {intent_threshold})\")\n",
    "            else:\n",
    "                print(f\"   • Intent Resolution:  {intent}\")\n",
    "            \n",
    "            if isinstance(task, (int, float)):\n",
    "                color = get_score_color(task, task_threshold)\n",
    "                reset = reset_color()\n",
    "                indicator = get_score_indicator(task, task_threshold)\n",
    "                print(f\"   {indicator} Task Adherence:     {color}{task:.1f}/5.0{reset} (임계값: {task_threshold})\")\n",
    "            else:\n",
    "                print(f\"   • Task Adherence:     {task}\")\n",
    "            \n",
    "            print(f\"   • Tool Call Accuracy: {tool}\")\n",
    "            \n",
    "            # 평가 이유\n",
    "            print(\"\\n💡 평가 상세:\")\n",
    "            \n",
    "            intent_reason = row.get(\"outputs.intent_resolution.intent_resolution_reason\", \"\")\n",
    "            task_reason = row.get(\"outputs.task_adherence.task_adherence_reason\", \"\")\n",
    "            tool_reason = row.get(\"outputs.tool_call_accuracy.tool_call_accuracy_reason\", \"\")\n",
    "            \n",
    "            if intent_reason:\n",
    "                print(\"\\n   [Intent Resolution 평가 이유]\")\n",
    "                for sentence in intent_reason.split(\". \"):\n",
    "                    if sentence.strip():\n",
    "                        print(f\"   • {sentence.strip()}.\")\n",
    "            \n",
    "            if task_reason:\n",
    "                print(\"\\n   [Task Adherence 평가 이유]\")\n",
    "                for sentence in task_reason.split(\". \"):\n",
    "                    if sentence.strip():\n",
    "                        print(f\"   • {sentence.strip()}.\")\n",
    "            \n",
    "            if tool_reason:\n",
    "                print(\"\\n   [Tool Call Accuracy 평가 이유]\")\n",
    "                for sentence in tool_reason.split(\". \"):\n",
    "                    if sentence.strip():\n",
    "                        print(f\"   • {sentence.strip()}.\")\n",
    "            \n",
    "            duration = row.get(\"outputs.operational_metrics.client-run-duration-in-seconds\", 0)\n",
    "            prompt_tokens = row.get(\"outputs.operational_metrics.prompt-tokens\", 0)\n",
    "            completion_tokens = row.get(\"outputs.operational_metrics.completion-tokens\", 0)\n",
    "            \n",
    "            print(\"\\n⏱️  성능 메트릭:\")\n",
    "            print(f\"   • 실행 시간: {duration:.2f}초\")\n",
    "            print(f\"   • 토큰 사용: {prompt_tokens:,} (입력) + {completion_tokens:,} (출력) = {prompt_tokens + completion_tokens:,} (총)\")\n",
    "            \n",
    "            issues = []\n",
    "            if isinstance(intent, (int, float)) and intent < intent_threshold:\n",
    "                issues.append(f\"Intent Resolution 점수 낮음 ({intent:.1f} < {intent_threshold})\")\n",
    "            if isinstance(task, (int, float)) and task < task_threshold:\n",
    "                issues.append(f\"Task Adherence 점수 낮음 ({task:.1f} < {task_threshold})\")\n",
    "            \n",
    "            if issues:\n",
    "                print(f\"\\n{get_score_color(1.0, 3.0)}⚠️  발견된 문제:{reset_color()}\")\n",
    "                for issue in issues:\n",
    "                    print(f\"   • {issue}\")\n",
    "        \n",
    "        # 섹션 4: 통계 요약\n",
    "        print(f\"\\n{SEPARATOR}\\n\")\n",
    "        print(\"\\n📈 통계 요약 및 분석\")\n",
    "        print(LINE)\n",
    "        \n",
    "        intent_scores = []\n",
    "        task_scores = []\n",
    "        durations = []\n",
    "        total_tokens_list = []\n",
    "        failed_queries = []\n",
    "        \n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            intent = row.get(\"outputs.intent_resolution.intent_resolution\")\n",
    "            task = row.get(\"outputs.task_adherence.task_adherence\")\n",
    "            duration = row.get(\"outputs.operational_metrics.client-run-duration-in-seconds\", 0)\n",
    "            prompt = row.get(\"outputs.operational_metrics.prompt-tokens\", 0)\n",
    "            completion = row.get(\"outputs.operational_metrics.completion-tokens\", 0)\n",
    "            \n",
    "            intent_threshold = row.get(\"outputs.intent_resolution.intent_resolution_threshold\", 3)\n",
    "            task_threshold = row.get(\"outputs.task_adherence.task_adherence_threshold\", 3)\n",
    "            \n",
    "            if isinstance(intent, (int, float)):\n",
    "                intent_scores.append(intent)\n",
    "                if intent < intent_threshold:\n",
    "                    query = extract_query_text(row.get(\"inputs.query\", []))\n",
    "                    failed_queries.append((idx, \"Intent Resolution\", intent, query[:50]))\n",
    "            \n",
    "            if isinstance(task, (int, float)):\n",
    "                task_scores.append(task)\n",
    "                if task < task_threshold:\n",
    "                    query = extract_query_text(row.get(\"inputs.query\", []))\n",
    "                    failed_queries.append((idx, \"Task Adherence\", task, query[:50]))\n",
    "            \n",
    "            if duration:\n",
    "                durations.append(duration)\n",
    "            total_tokens_list.append(prompt + completion)\n",
    "        \n",
    "        if intent_scores:\n",
    "            avg_intent = sum(intent_scores) / len(intent_scores)\n",
    "            color = get_score_color(avg_intent, 3.0)\n",
    "            reset = reset_color()\n",
    "            pass_count = len([s for s in intent_scores if s >= 3.0])\n",
    "            \n",
    "            print(\"\\n📊 Intent Resolution (의도 파악)\")\n",
    "            print(f\"   평균: {color}{avg_intent:.2f}/5.0{reset}\")\n",
    "            print(f\"   최고: {max(intent_scores):.1f}  |  최저: {min(intent_scores):.1f}\")\n",
    "            print(f\"   합격률: {pass_count}/{len(intent_scores)} ({pass_count/len(intent_scores)*100:.1f}%)\")\n",
    "        \n",
    "        if task_scores:\n",
    "            avg_task = sum(task_scores) / len(task_scores)\n",
    "            color = get_score_color(avg_task, 3.0)\n",
    "            reset = reset_color()\n",
    "            pass_count = len([s for s in task_scores if s >= 3.0])\n",
    "            \n",
    "            print(\"\\n📊 Task Adherence (작업 충실도)\")\n",
    "            print(f\"   평균: {color}{avg_task:.2f}/5.0{reset}\")\n",
    "            print(f\"   최고: {max(task_scores):.1f}  |  최저: {min(task_scores):.1f}\")\n",
    "            print(f\"   합격률: {pass_count}/{len(task_scores)} ({pass_count/len(task_scores)*100:.1f}%)\")\n",
    "        \n",
    "        if durations:\n",
    "            print(\"\\n⏱️  실행 시간\")\n",
    "            print(f\"   평균: {sum(durations)/len(durations):.2f}초\")\n",
    "            print(f\"   최대: {max(durations):.2f}초  |  최소: {min(durations):.2f}초\")\n",
    "        \n",
    "        if total_tokens_list:\n",
    "            avg_tokens = sum(total_tokens_list) / len(total_tokens_list)\n",
    "            total_all_tokens = sum(total_tokens_list)\n",
    "            \n",
    "            print(\"\\n💰 토큰 사용량\")\n",
    "            print(f\"   평균: {avg_tokens:,.0f} tokens/query\")\n",
    "            print(f\"   총합: {total_all_tokens:,} tokens\")\n",
    "            print(f\"   최대: {max(total_tokens_list):,}  |  최소: {min(total_tokens_list):,}\")\n",
    "            print(f\"   예상 비용 (GPT-4o): ${(total_all_tokens / 1000) * 0.0025:.4f}\")\n",
    "        \n",
    "        if failed_queries:\n",
    "            print(f\"\\n{get_score_color(1.0, 3.0)}⚠️  개선이 필요한 쿼리 ({len(failed_queries)}개){reset_color()}\")\n",
    "            print(SEPARATOR)\n",
    "            \n",
    "            seen = set()\n",
    "            for idx, metric, score, query in failed_queries:\n",
    "                key = (idx, metric)\n",
    "                if key not in seen:\n",
    "                    seen.add(key)\n",
    "                    print(f\"   Query #{idx}: {metric} = {score:.1f}\")\n",
    "                    print(f\"   └─ {query}...\")\n",
    "                    print()\n",
    "        else:\n",
    "            print(\"\\n✅ 모든 쿼리가 임계값을 통과했습니다!\")\n",
    "    \n",
    "    print(f\"\\n{LINE}\")\n",
    "    print(f\"✅ 총 {len(rows)}개 쿼리 평가 완료\")\n",
    "    print(f\"📁 상세 JSON: {eval_output_path.absolute()}\")\n",
    "    print(f\"💡 터미널에서도 실행 가능: python3 show_eval_results.py\")\n",
    "    print(f\"{LINE}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4900276",
   "metadata": {},
   "source": [
    "## 7. Evaluation 메트릭 해석 가이드\n",
    "\n",
    "### 메트릭 해석\n",
    "\n",
    "**Operational Metrics:**\n",
    "- `server-run-duration-in-seconds`: 서버에서 Agent 실행 시간\n",
    "- `client-run-duration-in-seconds`: 클라이언트에서 측정한 총 소요 시간\n",
    "- `prompt-tokens`: 입력 토큰 수\n",
    "- `completion-tokens`: 생성된 토큰 수\n",
    "\n",
    "**Performance Metrics:**\n",
    "- `tool_call_accuracy.*`: 도구 호출 정확도 (1-5점)\n",
    "- `intent_resolution.*`: 의도 파악 정확도 (1-5점)\n",
    "- `task_adherence.*`: 작업 준수도 (1-5점)\n",
    "\n",
    "**점수 해석:**\n",
    "- 5점: 완벽함\n",
    "- 4점: 좋음\n",
    "- 3점: 보통\n",
    "- 2점: 개선 필요\n",
    "- 1점: 매우 나쁨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1fd41",
   "metadata": {},
   "source": [
    "## 8. 다음 단계 및 Agent 개선\n",
    "\n",
    "### Agent 개선 방법\n",
    "\n",
    "1. **낮은 점수 분석**\n",
    "   - 어떤 쿼리에서 점수가 낮았는지 확인\n",
    "   - 어떤 evaluator에서 문제가 발생했는지 파악\n",
    "\n",
    "2. **프롬프트 개선**\n",
    "   - Agent instructions를 더 명확하게 작성\n",
    "   - 예시 추가\n",
    "   - 제약사항 명시\n",
    "\n",
    "3. **기능 개선**\n",
    "   - Tool 추가 또는 개선\n",
    "   - RAG 지식 베이스 보강\n",
    "   - Multi-Agent 조정\n",
    "\n",
    "4. **재평가**\n",
    "   - 동일한 쿼리로 재평가\n",
    "   - 점수 변화 비교\n",
    "   - 지속적인 개선\n",
    "\n",
    "### 참고 자료\n",
    "\n",
    "- [Azure AI Foundry Agent Evaluation](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)\n",
    "- [Built-in Evaluators](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk)\n",
    "- [Evaluation Best Practices](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-approach-gen-ai)\n",
    "\n",
    "---\n",
    "\n",
    "## 완료!\n",
    "\n",
    "축하합니다! Agent Evaluation을 성공적으로 완료했습니다. 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eea5182",
   "metadata": {},
   "source": [
    "# 06. Agent Evaluation (Agent í‰ê°€)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë°°í¬ëœ Agentì˜ í’ˆì§ˆê³¼ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©í‘œ\n",
    "\n",
    "- Azure AI Evaluation SDKë¥¼ ì‚¬ìš©í•œ Agent í‰ê°€\n",
    "- ì„±ëŠ¥, ì •í™•ë„, ì•ˆì „ì„± ë©”íŠ¸ë¦­ ìˆ˜ì§‘\n",
    "- í‰ê°€ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”\n",
    "\n",
    "## ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
    "\n",
    "1. Notebook 01-03 ì™„ë£Œ (Azure ë¦¬ì†ŒìŠ¤ ë° Agent ë°°í¬)\n",
    "2. `config.json` íŒŒì¼ ì¡´ì¬\n",
    "3. Azure AI Project ì ‘ê·¼ ê¶Œí•œ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784224b9",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° Config ë¡œë“œ\n",
    "\n",
    "ì´ì „ ë…¸íŠ¸ë¶ì—ì„œ ìƒì„±í•œ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eebcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# config.json ë¡œë“œ\n",
    "config_path = Path(\"config.json\")\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(\"config.json not found. Please run notebooks 01-03 first.\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# ì£¼ìš” ë³€ìˆ˜ ì¶”ì¶œ\n",
    "PROJECT_CONNECTION_STRING = config.get(\"project_connection_string\", \"\")\n",
    "simple_project_conn = PROJECT_CONNECTION_STRING.split(';')[0] if PROJECT_CONNECTION_STRING else \"\"\n",
    "\n",
    "print(\"=== Configuration Loaded ===\")\n",
    "print(f\"Resource Group: {config.get('resource_group')}\")\n",
    "print(f\"Location: {config.get('location')}\")\n",
    "print(f\"Project: {simple_project_conn}\")\n",
    "print(f\"Model: {config.get('model_deployment_name')}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc8337",
   "metadata": {},
   "source": [
    "## 2. Azure AI Evaluation íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "\n",
    "Agent í‰ê°€ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Evaluation íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "print(\"=== Installing Azure AI Evaluation Package ===\\n\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"azure-ai-evaluation\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"âœ… azure-ai-evaluation installed successfully\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Installation warning: {result.stderr}\")\n",
    "    print(\"   Proceeding anyway...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b155d",
   "metadata": {},
   "source": [
    "## 3. Evaluation ê°œìš”\n",
    "\n",
    "### Evaluation ìœ í˜•\n",
    "\n",
    "**Performance Evaluators (ì„±ëŠ¥ í‰ê°€):**\n",
    "- **Intent Resolution**: Agentê°€ ì‚¬ìš©ì ì˜ë„ë¥¼ ì˜¬ë°”ë¥´ê²Œ íŒŒì•…í–ˆëŠ”ì§€ í‰ê°€\n",
    "- **Tool Call Accuracy**: Agentê°€ ì˜¬ë°”ë¥¸ ë„êµ¬ë¥¼ ì •í™•í•˜ê²Œ í˜¸ì¶œí–ˆëŠ”ì§€ í‰ê°€  \n",
    "- **Task Adherence**: Agentê°€ ì§€ì‹œì‚¬í•­ì„ ì¶©ì‹¤íˆ ë”°ëëŠ”ì§€ í‰ê°€\n",
    "\n",
    "**Safety Evaluators (ì•ˆì „ì„± í‰ê°€):**\n",
    "- **Content Safety**: ë¶€ì ì ˆí•œ ì½˜í…ì¸ (í­ë ¥, í˜ì˜¤ ë“±) í¬í•¨ ì—¬ë¶€ í‰ê°€\n",
    "- **Indirect Attack**: ê°„ì ‘ì ì¸ ì•…ì˜ì  ê³µê²© ì‹œë„ ê°ì§€\n",
    "- **Code Vulnerability**: ìƒì„±ëœ ì½”ë“œì˜ ë³´ì•ˆ ì·¨ì•½ì  í‰ê°€\n",
    "\n",
    "### Evaluation í”„ë¡œì„¸ìŠ¤\n",
    "\n",
    "1. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±\n",
    "2. Agent ìƒì„± ë° ì¿¼ë¦¬ ì‹¤í–‰\n",
    "3. ì‘ë‹µ ìˆ˜ì§‘ ë° ë©”íŠ¸ë¦­ ì¸¡ì •\n",
    "4. Evaluatorë¡œ í’ˆì§ˆ í‰ê°€\n",
    "5. ê²°ê³¼ ì €ì¥ ë° ë¶„ì„\n",
    "6. Agent ì •ë¦¬\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4deab",
   "metadata": {},
   "source": [
    "## 4. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±\n",
    "\n",
    "Agentì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•  ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evals ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "evals_dir = Path(\"evals\")\n",
    "evals_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=== Creating Evaluation Test Queries ===\\n\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜\n",
    "eval_queries = [\n",
    "    {\n",
    "        \"query\": \"ì•ˆë…•í•˜ì„¸ìš”. ê°€ì¡± ì—¬í–‰ì§€ë¥¼ ì¶”ì²œë°›ê³  ì‹¶ì–´ìš”.\",\n",
    "        \"ground-truth\": \"AgentëŠ” ì¸ì‚¬ì— ì‘ë‹µí•˜ê³ , ì—¬í–‰ì§€ ì¶”ì²œì„ ìœ„í•´ Research Agentë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ë¶€ì‚°ì˜ í˜„ì¬ ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ì˜¨ë„ì™€ ì²´ê°ì˜¨ë„ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.\",\n",
    "        \"ground-truth\": \"ë¶€ì‚°ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì œê³µí•´ì•¼ í•˜ë©°, ì˜¨ë„ì™€ ì²´ê°ì˜¨ë„ë¥¼ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ì œì£¼ë„ì—ì„œ ê°€ì¡±ê³¼ í•¨ê»˜ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì—¬í–‰ì§€ëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
    "        \"ground-truth\": \"ì œì£¼ë„ì˜ ê°€ì¡± ì¹œí™”ì ì¸ ì—¬í–‰ì§€ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤. ìì—° ëª…ì†Œ, ì²´í—˜ í™œë™, ì ‘ê·¼ì„± ë“±ì„ ê³ ë ¤í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ì„œí•‘í•  ìˆ˜ ìˆëŠ” í•´ë³€ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.\",\n",
    "        \"ground-truth\": \"ì„œí•‘ì´ ê°€ëŠ¥í•œ í•œêµ­ì˜ í•´ë³€ ì—¬í–‰ì§€ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤. ì–‘ì–‘, ë¶€ì‚° ë“±ì˜ ì„œí•‘ ëª…ì†Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"ê°€ì„ì— ê°€ê¸° ì¢‹ì€ ë‹¨í’ ëª…ì†ŒëŠ” ì–´ë””ì¸ê°€ìš”?\",\n",
    "        \"ground-truth\": \"ê°€ì„ ê³„ì ˆì— ë°©ë¬¸í•˜ê¸° ì¢‹ì€ ë‹¨í’ ëª…ì†Œë¥¼ ê²€ìƒ‰í•˜ì—¬ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤. ë‚´ì¥ì‚°, ì„¤ì•…ì‚° ë“±ì˜ ìì—° ëª…ì†Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "with open(eval_queries_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(eval_queries, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Created {eval_queries_path}\")\n",
    "print(f\"\\nğŸ“‹ Test Queries ({len(eval_queries)} total):\\n\")\n",
    "\n",
    "for i, query in enumerate(eval_queries, 1):\n",
    "    print(f\"   {i}. {query['query'][:60]}...\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ê° ì¿¼ë¦¬ëŠ” Agentì˜ ë‹¤ë¥¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:\")\n",
    "print(\"   â€¢ ì¼ë°˜ ëŒ€í™” ë° ì—¬í–‰ ì˜ë„ íŒŒì•…\")\n",
    "print(\"   â€¢ ë‚ ì”¨ ì¡°íšŒ (Tool ê¸°ëŠ¥)\")\n",
    "print(\"   â€¢ ì—¬í–‰ì§€ ì§€ì‹ ê²€ìƒ‰ (RAG ê¸°ëŠ¥)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded78b6",
   "metadata": {},
   "source": [
    "## 5. Agent Evaluation ì‹¤í–‰\n",
    "\n",
    "í‰ê°€ìš© Agentë¥¼ ìƒì„±í•˜ê³  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•œ í›„, Evaluatorë¡œ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì°¸ê³ ì‚¬í•­\n",
    "\n",
    "- **ì‹¤í–‰ ì‹œê°„**: ì•½ 2-3ë¶„ ì†Œìš”\n",
    "- **ë¦¬ì „ ì œì•½**: eastus ë¦¬ì „ì—ì„œëŠ” ì¼ë¶€ Safety Evaluatorê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\n",
    "- **ê¶Œí•œ ì´ìŠˆ**: Azure AI Projectì˜ ìŠ¤í† ë¦¬ì§€ ê¶Œí•œì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Evaluation ì‹¤í–‰\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from azure.ai.agents.models import RunStatus, MessageRole\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.evaluation import (\n",
    "    AIAgentConverter, evaluate, ToolCallAccuracyEvaluator, IntentResolutionEvaluator, \n",
    "    TaskAdherenceEvaluator\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "print(\"=== Running Agent Evaluation ===\\n\")\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "current_dir = Path(\".\")\n",
    "evals_dir = current_dir / \"evals\"\n",
    "eval_queries_path = evals_dir / \"eval-queries.json\"\n",
    "eval_input_path = evals_dir / \"eval-input.jsonl\"\n",
    "eval_output_path = evals_dir / \"eval-output.json\"\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ì´ë¯¸ configì—ì„œ ë¡œë“œë¨)\n",
    "project_endpoint = simple_project_conn\n",
    "parsed_endpoint = urlparse(project_endpoint)\n",
    "model_endpoint = f\"{parsed_endpoint.scheme}://{parsed_endpoint.netloc}\"\n",
    "deployment_name = config.get(\"model_deployment_name\", \"gpt-4o\")\n",
    "\n",
    "print(f\"ğŸ“‹ Configuration:\")\n",
    "print(f\"   Project: {project_endpoint}\")\n",
    "print(f\"   Model: {deployment_name}\")\n",
    "print(f\"   Test Queries: {eval_queries_path}\")\n",
    "print(f\"\\n\")\n",
    "\n",
    "# Initialize AIProjectClient\n",
    "print(\"ğŸ”Œ Connecting to AI Project...\")\n",
    "credential = DefaultAzureCredential()\n",
    "ai_project = AIProjectClient(\n",
    "    credential=credential,\n",
    "    endpoint=project_endpoint,\n",
    "    api_version=\"2025-05-15-preview\"  # Evaluations require preview API\n",
    ")\n",
    "print(\"âœ… Connected\\n\")\n",
    "\n",
    "# Evaluationìš© Agent ìƒì„±\n",
    "print(\"ğŸ¤– Creating Evaluation Agent...\")\n",
    "eval_agent = ai_project.agents.create_agent(\n",
    "    model=deployment_name,\n",
    "    name=\"Evaluation Agent\",\n",
    "    instructions=\"\"\"You are a helpful travel and weather assistant.\n",
    "    \n",
    "You can help users with:\n",
    "1. Travel recommendations and destination information\n",
    "2. Weather information for any city\n",
    "3. General travel planning advice\n",
    "\n",
    "Be friendly, informative, and provide detailed responses.\"\"\"\n",
    ")\n",
    "print(f\"âœ… Agent created: {eval_agent.name} (ID: {eval_agent.id})\\n\")\n",
    "\n",
    "# Setup evaluation config\n",
    "api_version = config.get(\"api_version\", \"2024-08-01-preview\")\n",
    "model_config = {\n",
    "    \"azure_deployment\": deployment_name,\n",
    "    \"azure_endpoint\": model_endpoint,\n",
    "    \"api_version\": api_version,\n",
    "}\n",
    "\n",
    "thread_data_converter = AIAgentConverter(ai_project)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰ ë° evaluation input ì¤€ë¹„\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“ Executing Test Queries\\n\")\n",
    "\n",
    "with open(eval_queries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"   Total queries: {len(test_data)}\\n\")\n",
    "\n",
    "with open(eval_input_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, row in enumerate(test_data, 1):\n",
    "        query_text = row.get(\"query\")\n",
    "        print(f\"   [{idx}/{len(test_data)}] {query_text[:50]}...\")\n",
    "        \n",
    "        # ìƒˆ ìŠ¤ë ˆë“œ ìƒì„± (ê° ì¿¼ë¦¬ë¥¼ ê²©ë¦¬)\n",
    "        thread = ai_project.agents.threads.create()\n",
    "        \n",
    "        # ì‚¬ìš©ì ì¿¼ë¦¬ ìƒì„±\n",
    "        ai_project.agents.messages.create(\n",
    "            thread.id, role=MessageRole.USER, content=query_text\n",
    "        )\n",
    "        \n",
    "        # Agent ì‹¤í–‰ ë° ì„±ëŠ¥ ì¸¡ì •\n",
    "        start_time = time.time()\n",
    "        run = ai_project.agents.runs.create_and_process(\n",
    "            thread_id=thread.id, agent_id=eval_agent.id\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if run.status != RunStatus.COMPLETED:\n",
    "            print(f\"      âš ï¸  Run failed: {run.last_error or 'Unknown error'}\")\n",
    "            continue\n",
    "        \n",
    "        # ìš´ì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘\n",
    "        operational_metrics = {\n",
    "            \"server-run-duration-in-seconds\": (\n",
    "                run.completed_at - run.created_at\n",
    "            ).total_seconds(),\n",
    "            \"client-run-duration-in-seconds\": end_time - start_time,\n",
    "            \"completion-tokens\": run.usage.completion_tokens,\n",
    "            \"prompt-tokens\": run.usage.prompt_tokens,\n",
    "            \"ground-truth\": row.get(\"ground-truth\", '')\n",
    "        }\n",
    "        \n",
    "        # Thread ë°ì´í„° + ìš´ì˜ ë©”íŠ¸ë¦­ì„ evaluation inputì— ì¶”ê°€\n",
    "        evaluation_data = thread_data_converter.prepare_evaluation_data(thread_ids=thread.id)\n",
    "        eval_item = evaluation_data[0]\n",
    "        eval_item[\"metrics\"] = operational_metrics\n",
    "        f.write(json.dumps(eval_item, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"      âœ… Completed in {operational_metrics['client-run-duration-in-seconds']:.1f}s\")\n",
    "        print(f\"         Tokens: {operational_metrics['prompt-tokens']} prompt + {operational_metrics['completion-tokens']} completion\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâœ… All queries executed successfully\")\n",
    "print(f\"   Input saved to: {eval_input_path}\\n\")\n",
    "\n",
    "# Evaluation ì‹¤í–‰\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ”¬ Running Evaluators\\n\")\n",
    "\n",
    "print(\"   Evaluators:\")\n",
    "print(\"      â€¢ ToolCallAccuracyEvaluator\")\n",
    "print(\"      â€¢ IntentResolutionEvaluator\")\n",
    "print(\"      â€¢ TaskAdherenceEvaluator\")\n",
    "print(\"\\n   âš ï¸  ì°¸ê³ : eastus ë¦¬ì „ì—ì„œëŠ” ì¼ë¶€ RAI í‰ê°€ìê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "print(\"      (CodeVulnerability, ContentSafety, IndirectAttackëŠ” ì œì™¸)\\n\")\n",
    "print(\"   â³ This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# OperationalMetricsEvaluator ì •ì˜\n",
    "class OperationalMetricsEvaluator:\n",
    "    \"\"\"Propagate operational metrics to the final evaluation results\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, *, metrics: dict, **kwargs):\n",
    "        return metrics\n",
    "\n",
    "# Evaluation ì‹¤í–‰ (ë¡œì»¬)\n",
    "print(\"   ğŸ’¡ í‰ê°€ ê²°ê³¼ë¥¼ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "results = evaluate(\n",
    "    evaluation_name=\"foundry-agent-evaluation\",\n",
    "    data=eval_input_path,\n",
    "    evaluators={\n",
    "        \"operational_metrics\": OperationalMetricsEvaluator(),\n",
    "        \"tool_call_accuracy\": ToolCallAccuracyEvaluator(model_config=model_config),\n",
    "        \"intent_resolution\": IntentResolutionEvaluator(model_config=model_config),\n",
    "        \"task_adherence\": TaskAdherenceEvaluator(model_config=model_config),\n",
    "        # eastus ë¦¬ì „ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠëŠ” í‰ê°€ìë“¤ì€ ì œì™¸\n",
    "        # \"code_vulnerability\": CodeVulnerabilityEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"content_safety\": ContentSafetyEvaluator(credential=credential, azure_ai_project=project_endpoint),\n",
    "        # \"indirect_attack\": IndirectAttackEvaluator(credential=credential, azure_ai_project=project_endpoint)\n",
    "    },\n",
    "    output_path=eval_output_path,\n",
    "    # azure_ai_project íŒŒë¼ë¯¸í„° ì œê±° (ML workspace ë¶ˆí•„ìš”)\n",
    ")\n",
    "\n",
    "print(\"âœ… Evaluation completed!\\n\")\n",
    "print(f\"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {eval_output_path}\")\n",
    "print(f\"   ë‹¤ìŒ ì…€ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluation Agent ì‚­ì œ\n",
    "print(\"ğŸ§¹ Cleaning up...\")\n",
    "ai_project.agents.delete_agent(eval_agent.id)\n",
    "print(f\"âœ… Evaluation Agent deleted: {eval_agent.id}\\n\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dbb91",
   "metadata": {},
   "source": [
    "## 6. Evaluation ê²°ê³¼ ì‹œê°í™”\n",
    "\n",
    "ê²°ê³¼ë¥¼ í‘œì™€ ì°¨íŠ¸ë¡œ ì‹œê°í™”í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation ê²°ê³¼ ì‹œê°í™”\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== ğŸ“Š Evaluation Results (í‰ê°€ ê²°ê³¼) ===\\n\")\n",
    "\n",
    "# íŒŒì¼ì—ì„œ ê²°ê³¼ ë¡œë“œ\n",
    "eval_output_path = Path(\"evals/eval-output.json\")\n",
    "\n",
    "if not eval_output_path.exists():\n",
    "    print(\"âŒ í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"   íŒŒì¼ ê²½ë¡œ: {eval_output_path}\")\n",
    "    print(\"\\n   ë¨¼ì € Cell 5ë¥¼ ì‹¤í–‰í•˜ì—¬ í‰ê°€ë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.\\n\")\n",
    "else:\n",
    "    with open(eval_output_path, 'r', encoding='utf-8') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    metrics = results.get('metrics', {})\n",
    "    rows = results.get('rows', [])\n",
    "    \n",
    "    # 1. ì „ì²´ ì ìˆ˜ ìš”ì•½\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"â­ OVERALL PERFORMANCE SCORES (ì „ì²´ ì„±ëŠ¥ ì ìˆ˜)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    performance_scores = {}\n",
    "    for key, value in metrics.items():\n",
    "        if any(x in key.lower() for x in ['accuracy', 'resolution', 'adherence']) and isinstance(value, (int, float)):\n",
    "            # ì ìˆ˜ ì´ë¦„ ì •ë¦¬\n",
    "            score_name = key.replace('.', ' ').replace('_', ' ').title()\n",
    "            performance_scores[score_name] = value\n",
    "    \n",
    "    if performance_scores:\n",
    "        for score_name, score in sorted(performance_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "            # 1-5 ì ìˆ˜ë¥¼ ë³„í‘œì™€ í”„ë¡œê·¸ë ˆìŠ¤ ë°”ë¡œ í‘œì‹œ\n",
    "            stars = 'â˜…' * int(score) + 'â˜†' * (5 - int(score))\n",
    "            bar_length = int(score * 10)\n",
    "            bar = 'â–ˆ' * bar_length + 'â–‘' * (50 - bar_length)\n",
    "            print(f\"{score_name:.<40} {score:.2f}/5.0  {stars}\")\n",
    "            print(f\"{'':.<40} {bar}\\n\")\n",
    "    else:\n",
    "        print(\"   (ì„±ëŠ¥ ì ìˆ˜ ë°ì´í„° ì—†ìŒ)\\n\")\n",
    "    \n",
    "    # 2. ìš´ì˜ ë©”íŠ¸ë¦­ ìš”ì•½\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âš¡ OPERATIONAL METRICS (ìš´ì˜ ë©”íŠ¸ë¦­)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    operational_data = {}\n",
    "    for key, value in metrics.items():\n",
    "        if 'duration' in key.lower():\n",
    "            operational_data[f\"â±ï¸  {key.replace('_', ' ').replace('-', ' ').title()}\"] = f\"{value:.2f}s\"\n",
    "        elif 'token' in key.lower():\n",
    "            operational_data[f\"ğŸ”¤ {key.replace('_', ' ').replace('-', ' ').title()}\"] = f\"{int(value):,}\"\n",
    "    \n",
    "    if operational_data:\n",
    "        for metric_name, value in operational_data.items():\n",
    "            print(f\"   {metric_name:.<65} {value:>10}\")\n",
    "    else:\n",
    "        print(\"   (ìš´ì˜ ë©”íŠ¸ë¦­ ë°ì´í„° ì—†ìŒ)\")\n",
    "    \n",
    "    # 3. ê°œë³„ ì¿¼ë¦¬ ê²°ê³¼ ìƒì„¸ í…Œì´ë¸”\n",
    "    print(\"\\n\\n\" + \"=\"*120)\n",
    "    print(\"ğŸ“‹ DETAILED QUERY RESULTS (ì¿¼ë¦¬ë³„ ìƒì„¸ ê²°ê³¼)\")\n",
    "    print(\"=\"*120 + \"\\n\")\n",
    "    \n",
    "    if rows:\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # ì£¼ìš” ì»¬ëŸ¼ ì„ íƒ ë° ì •ë¦¬\n",
    "        display_data = []\n",
    "        for idx, row in enumerate(rows, 1):\n",
    "            query_text = row.get('query', '')[:45] + '...' if len(row.get('query', '')) > 45 else row.get('query', '')\n",
    "            \n",
    "            result_row = {\n",
    "                'No.': idx,\n",
    "                'Query': query_text,\n",
    "                'Intent': row.get('intent_resolution.intent_resolution', '-'),\n",
    "                'Task': row.get('task_adherence.task_adherence', '-'),\n",
    "                'Tool': row.get('tool_call_accuracy.tool_call_accuracy', '-'),\n",
    "                'Duration(s)': f\"{row.get('client-run-duration-in-seconds', 0):.1f}\",\n",
    "                'Tokens': f\"{row.get('prompt-tokens', 0)}+{row.get('completion-tokens', 0)}\"\n",
    "            }\n",
    "            display_data.append(result_row)\n",
    "        \n",
    "        result_df = pd.DataFrame(display_data)\n",
    "        print(result_df.to_string(index=False))\n",
    "        \n",
    "        # í‰ê·  ì ìˆ˜ ê³„ì‚°\n",
    "        print(\"\\n\" + \"-\"*120)\n",
    "        intent_scores = [r.get('intent_resolution.intent_resolution') for r in rows if isinstance(r.get('intent_resolution.intent_resolution'), (int, float))]\n",
    "        task_scores = [r.get('task_adherence.task_adherence') for r in rows if isinstance(r.get('task_adherence.task_adherence'), (int, float))]\n",
    "        tool_scores = [r.get('tool_call_accuracy.tool_call_accuracy') for r in rows if isinstance(r.get('tool_call_accuracy.tool_call_accuracy'), (int, float))]\n",
    "        \n",
    "        if intent_scores:\n",
    "            print(f\"\\nğŸ“ˆ Average Scores:\")\n",
    "            if intent_scores: print(f\"   Intent Resolution:  {sum(intent_scores)/len(intent_scores):.2f}/5.0\")\n",
    "            if task_scores: print(f\"   Task Adherence:     {sum(task_scores)/len(task_scores):.2f}/5.0\")\n",
    "            if tool_scores: print(f\"   Tool Call Accuracy: {sum(tool_scores)/len(tool_scores):.2f}/5.0\")\n",
    "    else:\n",
    "        print(\"   (ì¿¼ë¦¬ ê²°ê³¼ ì—†ìŒ)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    \n",
    "    # 4. ì ìˆ˜ ë¶„í¬ ì°¨íŠ¸\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“Š SCORE DISTRIBUTION (ì ìˆ˜ ë¶„í¬)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    if rows:\n",
    "        # ê° ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ë¶„í¬ ê³„ì‚°\n",
    "        for metric_name, scores in [\n",
    "            ('Intent Resolution', intent_scores),\n",
    "            ('Task Adherence', task_scores),\n",
    "            ('Tool Call Accuracy', tool_scores)\n",
    "        ]:\n",
    "            if scores:\n",
    "                avg_score = sum(scores) / len(scores)\n",
    "                print(f\"\\n{metric_name}:\")\n",
    "                \n",
    "                # íˆìŠ¤í† ê·¸ë¨\n",
    "                for score in range(5, 0, -1):\n",
    "                    count = sum(1 for s in scores if int(s) == score)\n",
    "                    bar = 'â–ˆ' * count + 'â–‘' * (len(scores) - count)\n",
    "                    print(f\"   {score} {'â˜…' * score:5}  [{count:2}]  {bar}\")\n",
    "                \n",
    "                print(f\"   {'â”€' * 40}\")\n",
    "                print(f\"   Average: {avg_score:.2f}/5.0\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # 5. íŒŒì¼ ì €ì¥ ìœ„ì¹˜\n",
    "    print(f\"\\n\\nâœ… í‰ê°€ ì™„ë£Œ!\")\n",
    "    print(f\"\\nğŸ“ ìƒì„¸ ê²°ê³¼ íŒŒì¼:\")\n",
    "    print(f\"   â€¢ {eval_output_path}\")\n",
    "    print(f\"   â€¢ evals/eval-input.jsonl\")\n",
    "    print(f\"\\nğŸ’¡ Tip: ìœ„ íŒŒì¼ì„ ì—´ì–´ì„œ ë” ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4900276",
   "metadata": {},
   "source": [
    "## 7. Evaluation ë©”íŠ¸ë¦­ í•´ì„ ê°€ì´ë“œ\n",
    "\n",
    "### ë©”íŠ¸ë¦­ í•´ì„\n",
    "\n",
    "**Operational Metrics:**\n",
    "- `server-run-duration-in-seconds`: ì„œë²„ì—ì„œ Agent ì‹¤í–‰ ì‹œê°„\n",
    "- `client-run-duration-in-seconds`: í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì¸¡ì •í•œ ì´ ì†Œìš” ì‹œê°„\n",
    "- `prompt-tokens`: ì…ë ¥ í† í° ìˆ˜\n",
    "- `completion-tokens`: ìƒì„±ëœ í† í° ìˆ˜\n",
    "\n",
    "**Performance Metrics:**\n",
    "- `tool_call_accuracy.*`: ë„êµ¬ í˜¸ì¶œ ì •í™•ë„ (1-5ì )\n",
    "- `intent_resolution.*`: ì˜ë„ íŒŒì•… ì •í™•ë„ (1-5ì )\n",
    "- `task_adherence.*`: ì‘ì—… ì¤€ìˆ˜ë„ (1-5ì )\n",
    "\n",
    "**ì ìˆ˜ í•´ì„:**\n",
    "- 5ì : ì™„ë²½í•¨\n",
    "- 4ì : ì¢‹ìŒ\n",
    "- 3ì : ë³´í†µ\n",
    "- 2ì : ê°œì„  í•„ìš”\n",
    "- 1ì : ë§¤ìš° ë‚˜ì¨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1fd41",
   "metadata": {},
   "source": [
    "## 8. ë‹¤ìŒ ë‹¨ê³„ ë° Agent ê°œì„ \n",
    "\n",
    "### Agent ê°œì„  ë°©ë²•\n",
    "\n",
    "1. **ë‚®ì€ ì ìˆ˜ ë¶„ì„**\n",
    "   - ì–´ë–¤ ì¿¼ë¦¬ì—ì„œ ì ìˆ˜ê°€ ë‚®ì•˜ëŠ”ì§€ í™•ì¸\n",
    "   - ì–´ë–¤ evaluatorì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆëŠ”ì§€ íŒŒì•…\n",
    "\n",
    "2. **í”„ë¡¬í”„íŠ¸ ê°œì„ **\n",
    "   - Agent instructionsë¥¼ ë” ëª…í™•í•˜ê²Œ ì‘ì„±\n",
    "   - ì˜ˆì‹œ ì¶”ê°€\n",
    "   - ì œì•½ì‚¬í•­ ëª…ì‹œ\n",
    "\n",
    "3. **ê¸°ëŠ¥ ê°œì„ **\n",
    "   - Tool ì¶”ê°€ ë˜ëŠ” ê°œì„ \n",
    "   - RAG ì§€ì‹ ë² ì´ìŠ¤ ë³´ê°•\n",
    "   - Multi-Agent ì¡°ì •\n",
    "\n",
    "4. **ì¬í‰ê°€**\n",
    "   - ë™ì¼í•œ ì¿¼ë¦¬ë¡œ ì¬í‰ê°€\n",
    "   - ì ìˆ˜ ë³€í™” ë¹„êµ\n",
    "   - ì§€ì†ì ì¸ ê°œì„ \n",
    "\n",
    "### ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [Azure AI Foundry Agent Evaluation](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/agent-evaluate-sdk)\n",
    "- [Built-in Evaluators](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk)\n",
    "- [Evaluation Best Practices](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-approach-gen-ai)\n",
    "\n",
    "---\n",
    "\n",
    "## ì™„ë£Œ!\n",
    "\n",
    "ì¶•í•˜í•©ë‹ˆë‹¤! Agent Evaluationì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

#!/usr/bin/env python3
"""
COMPLETE_GUIDE.mdë¥¼ ì²­í‚¹í•˜ì—¬ knowledge-base.json ìƒì„±
"""

import json
import re
from pathlib import Path
from typing import List, Dict

def parse_markdown_sections(md_content: str) -> List[Dict]:
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì„¹ì…˜ë³„ë¡œ íŒŒì‹±"""
    
    documents = []
    
    # ì£¼ìš” ì„¹ì…˜ íŒ¨í„´ (# 1., # 2., # 3., # 4., # 5.)
    sections = re.split(r'\n(# \d+\. .+?)\n', md_content)
    
    # ì²« ë²ˆì§¸ëŠ” í—¤ë” ë¶€ë¶„ (ëª©ì°¨ê¹Œì§€)
    intro = sections[0]
    
    # ë‚˜ë¨¸ì§€ëŠ” ì„¹ì…˜ë³„ë¡œ ì²˜ë¦¬
    for i in range(1, len(sections), 2):
        if i + 1 >= len(sections):
            break
            
        section_title = sections[i].replace('# ', '').strip()
        section_content = sections[i + 1].strip()
        
        # ì„¹ì…˜ ë²ˆí˜¸ ì¶”ì¶œ
        section_num = section_title.split('.')[0]
        section_name = section_title.split('. ', 1)[1]
        
        # í•˜ìœ„ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¦¬ (## ë¡œ ì‹œì‘)
        subsections = re.split(r'\n(## .+?)\n', section_content)
        
        # ì„¹ì…˜ ê°œìš” (ì²« ë¶€ë¶„)
        overview = subsections[0].strip()
        
        if overview:
            # ì„¹ì…˜ ê°œìš” ë¬¸ì„œ
            doc_id = f"doc-{section_num.zfill(2)}-00"
            documents.append({
                "id": doc_id,
                "title": f"{section_name} - ê°œìš”",
                "content": overview[:2000],  # ìµœëŒ€ 2000ì
                "category": section_name,
                "section": section_num,
                "subsection": "overview",
                "metadata": {
                    "source": "COMPLETE_GUIDE.md",
                    "lastModified": "2025-09-30",
                    "tags": get_tags_for_section(section_num)
                }
            })
        
        # í•˜ìœ„ ì„¹ì…˜ ì²˜ë¦¬
        subsection_counter = 1
        for j in range(1, len(subsections), 2):
            if j + 1 >= len(subsections):
                break
            
            subsection_title = subsections[j].replace('## ', '').strip()
            subsection_content = subsections[j + 1].strip()
            
            # ë„ˆë¬´ ê¸´ ê²½ìš° ë¶„í• 
            chunks = split_long_content(subsection_content, max_length=1500)
            
            for chunk_idx, chunk in enumerate(chunks):
                doc_id = f"doc-{section_num.zfill(2)}-{str(subsection_counter).zfill(2)}"
                if len(chunks) > 1:
                    doc_id += f"-{chr(97 + chunk_idx)}"  # a, b, c...
                
                documents.append({
                    "id": doc_id,
                    "title": f"{section_name} - {subsection_title}",
                    "content": chunk,
                    "category": section_name,
                    "section": section_num,
                    "subsection": subsection_title,
                    "metadata": {
                        "source": "COMPLETE_GUIDE.md",
                        "lastModified": "2025-09-30",
                        "tags": get_tags_for_section(section_num)
                    }
                })
            
            subsection_counter += 1
    
    return documents

def split_long_content(content: str, max_length: int = 1500) -> List[str]:
    """ê¸´ ì½˜í…ì¸ ë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• """
    if len(content) <= max_length:
        return [content]
    
    chunks = []
    paragraphs = content.split('\n\n')
    current_chunk = ""
    
    for para in paragraphs:
        if len(current_chunk) + len(para) + 2 <= max_length:
            current_chunk += para + "\n\n"
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = para + "\n\n"
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def get_tags_for_section(section_num: str) -> List[str]:
    """ì„¹ì…˜ ë²ˆí˜¸ì— ë”°ë¥¸ íƒœê·¸ ë°˜í™˜"""
    tag_map = {
        "1": ["agent-development", "azure-ai-foundry", "sdk", "basics"],
        "2": ["multi-agent", "architecture", "orchestration", "connected-agents"],
        "3": ["rag", "azure-search", "vector-search", "embeddings"],
        "4": ["mcp", "tools", "integration", "external-apis"],
        "5": ["container-apps", "deployment", "kubernetes", "devops"]
    }
    return tag_map.get(section_num, ["general"])

def main():
    # íŒŒì¼ ê²½ë¡œ
    project_root = Path(__file__).parent.parent
    md_file = project_root / "data" / "COMPLETE_GUIDE.md"
    json_file = project_root / "data" / "knowledge-base.json"
    
    print(f"ğŸ“– ì½ëŠ” ì¤‘: {md_file}")
    
    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸°
    with open(md_file, 'r', encoding='utf-8') as f:
        md_content = f.read()
    
    print("âœ‚ï¸  ì²­í‚¹ ì¤‘...")
    
    # ì„¹ì…˜ë³„ë¡œ íŒŒì‹±
    documents = parse_markdown_sections(md_content)
    
    print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ìƒì„±ë¨")
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(documents, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {json_file}")
    
    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š ë¬¸ì„œ í†µê³„:")
    for section_num in ["1", "2", "3", "4", "5"]:
        section_docs = [d for d in documents if d.get("section") == section_num]
        if section_docs:
            section_name = section_docs[0]["category"]
            print(f"  - ì„¹ì…˜ {section_num} ({section_name}): {len(section_docs)}ê°œ ë¬¸ì„œ")

if __name__ == "__main__":
    main()
